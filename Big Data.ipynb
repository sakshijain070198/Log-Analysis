{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9dc716c7-f167-4d00-a26a-3ae5d1f80b48",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "608cd269-e3bc-4951-b257-5179a379be3e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, explode, split, count, regexp_extract\n",
    "\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read Multiple CSV Files\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64159ba3-783d-43e9-ac4a-d2b54d8c1dc8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"dbfs:/FileStore/shared_uploads/tanmay.k.singh@sjsu.edu/anomaly_label.csv\")\n",
    "df2 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"dbfs:/FileStore/shared_uploads/tanmay.k.singh@sjsu.edu/Event_occurrence_matrix.csv\")\n",
    "df3 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"dbfs:/FileStore/shared_uploads/tanmay.k.singh@sjsu.edu/HDFS_log_templates.csv\")\n",
    "df4 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"dbfs:/FileStore/shared_uploads/tanmay.k.singh@sjsu.edu/Event_traces.csv\")\n",
    "hdfs_logs = spark.read.format(\"csv\").option(\"header\", \"false\").load(\"dbfs:/FileStore/shared_uploads/nikhilsarma.gudur@sjsu.edu/HDFS.log\")\n",
    "\n",
    "# Additional files uploaded\n",
    "# dbfs:/FileStore/shared_uploads/tanmay.k.singh@sjsu.edu/HDFS.npz\n",
    "# dbfs:/FileStore/shared_uploads/tanmay.k.singh@sjsu.edu/README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "097f6efb-a685-4cea-833b-5c057b175423",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly Label DataFrame:\n+------------------------+-------+\n|BlockId                 |Label  |\n+------------------------+-------+\n|blk_-1608999687919862906|Normal |\n|blk_7503483334202473044 |Normal |\n|blk_-3544583377289625738|Anomaly|\n|blk_-9073992586687739851|Normal |\n|blk_7854771516489510256 |Normal |\n|blk_1717858812220360316 |Normal |\n|blk_-2519617320378473615|Normal |\n|blk_7063315473424667801 |Normal |\n|blk_8586544123689943463 |Normal |\n|blk_2765344736980045501 |Normal |\n|blk_-2900490557492272760|Normal |\n|blk_-50273257731426871  |Normal |\n|blk_4394112519745907149 |Normal |\n|blk_3640100967125688321 |Normal |\n|blk_-40115644493265216  |Normal |\n|blk_-8531310335568756456|Anomaly|\n|blk_-3409923645141256069|Normal |\n|blk_3974948352784823938 |Normal |\n|blk_5647760196018207394 |Normal |\n|blk_-202775138379690649 |Normal |\n+------------------------+-------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Show the first few rows of each DataFrame to confirm they have been loaded correctly\n",
    "print(\"Anomaly Label DataFrame:\")\n",
    "df1.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2028c16f-8520-45c2-8764-2e65fac4f9b5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-4330738738621508>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[43mdf1\u001B[49m\u001B[38;5;241m.\u001B[39mselect(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBlockId\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mcount()\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'df1' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-4330738738621508>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mdf1\u001B[49m\u001B[38;5;241m.\u001B[39mselect(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBlockId\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mcount()\n\n\u001B[0;31mNameError\u001B[0m: name 'df1' is not defined",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'df1' is not defined",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df1.select(\"BlockId\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d634a431-b104-40fd-a5cb-f69d0c2a1212",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "575061\n"
     ]
    }
   ],
   "source": [
    "unique_blockid_count_df1 = df1.select(\"BlockId\").distinct().count()\n",
    "print(unique_blockid_count_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb9320eb-3ead-40b7-bd63-e53937c3fbcf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n|             BlockId|  Label|Type| E1| E2| E3| E4| E5| E6| E7| E8| E9|E10|E11|E12|E13|E14|E15|E16|E17|E18|E19|E20|E21|E22|E23|E24|E25|E26|E27|E28|E29|\n+--------------------+-------+----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n|blk_-160899968791...|Success|null|  0|  0|203|  0| 10|  7|  0|  0|  3|  0|  3|  0|  0|  0|  0|  4|  0|  4|  0|  0| 10|  1| 10|  0|  4| 10|  0|  0|  0|\n|blk_7503483334202...|Success|null|  0|  2|  1|  0|  3|  0|  0|  0|  3|  0|  3|  0|  0|  0|  0|  0|  0|  0|  0|  0|  3|  1|  3|  0|  0|  3|  0|  0|  0|\n|blk_-354458337728...|   Fail|  21|  0|  0|203|  0|  3|  0|  0|  0|  3|  0|  3|  0|  0|  0|  0|  0|  0|  0|  0|  1|  3|  1|  3|  0|  0|  3|  0|  0|  0|\n|blk_-907399258668...|Success|null|  0|  3|  0|  0|  3|  0|  0|  0|  3|  0|  3|  0|  0|  0|  0|  0|  0|  0|  0|  0|  3|  1|  3|  0|  0|  3|  0|  0|  0|\n|blk_7854771516489...|Success|null|  0|  3|  1| 15|  3|  0|  0|  0|  3|  0|  3|  0|  0|  0|  0|  0|  0|  0|  0|  0|  3|  1|  3|  0|  0|  3|  0|  0|  0|\n|blk_1717858812220...|Success|null|  0|  3|  1| 15|  3|  0|  0|  0|  3|  0|  3|  0|  0|  0|  0|  0|  0|  0|  0|  0|  3|  1|  3|  0|  0|  3|  0|  0|  0|\n|blk_-251961732037...|Success|null|  0|  3|  1| 15|  3|  0|  0|  0|  3|  0|  3|  0|  0|  0|  0|  0|  0|  0|  0|  0|  3|  1|  3|  0|  0|  3|  0|  0|  0|\n|blk_7063315473424...|Success|null|  0|  3|  1| 15|  3|  0|  0|  0|  3|  0|  3|  0|  0|  0|  0|  0|  0|  0|  0|  0|  3|  1|  3|  0|  0|  3|  0|  0|  0|\n|blk_8586544123689...|Success|null|  0|  3|  1| 15|  3|  0|  0|  0|  3|  0|  3|  0|  0|  0|  0|  0|  0|  0|  0|  0|  3|  1|  3|  0|  0|  3|  0|  0|  0|\n|blk_2765344736980...|Success|null|  0|  3|  1| 15|  3|  0|  0|  0|  3|  0|  3|  0|  0|  0|  0|  0|  0|  0|  0|  0|  3|  1|  3|  0|  0|  3|  0|  0|  0|\n|blk_-290049055749...|Success|null|  0|  3|  1| 15|  3|  0|  0|  0|  3|  0|  3|  0|  0|  0|  0|  0|  0|  0|  0|  0|  3|  1|  3|  0|  0|  3|  0|  0|  0|\n|blk_-502732577314...|Success|null|  0|  3|  1| 15|  3|  0|  0|  0|  3|  0|  3|  0|  0|  0|  0|  0|  0|  0|  0|  0|  3|  1|  3|  0|  0|  3|  0|  0|  0|\n|blk_4394112519745...|Success|null|  0|  3|  1| 15|  3|  0|  0|  0|  3|  0|  3|  0|  0|  0|  0|  0|  0|  0|  0|  0|  3|  1|  3|  0|  0|  3|  0|  0|  0|\n|blk_3640100967125...|Success|null|  0|  3|  1| 15|  3|  0|  0|  0|  3|  0|  3|  0|  0|  0|  0|  0|  0|  0|  0|  0|  3|  1|  3|  0|  0|  3|  0|  0|  0|\n|blk_-401156444932...|Success|null|  0|  3|  1| 15|  3|  0|  0|  0|  3|  0|  3|  0|  0|  0|  0|  0|  0|  0|  0|  0|  3|  1|  3|  0|  0|  3|  0|  0|  0|\n|blk_-853131033556...|   Fail|   4|  0|  3|  1| 15|  3|  0|  0|  0|  3|  0|  3|  0|  0|  0|  0|  0|  0|  0|  0|  0|  3|  1|  3|  0|  0|  4|  0|  1|  0|\n|blk_-340992364514...|Success|null|  0|  3|  1| 15|  3|  0|  0|  0|  3|  0|  3|  0|  0|  0|  0|  0|  0|  0|  0|  0|  3|  1|  3|  0|  0|  3|  0|  0|  0|\n|blk_3974948352784...|Success|null|  0|  3|  1| 15|  3|  0|  0|  0|  3|  0|  3|  0|  0|  0|  0|  0|  0|  0|  0|  0|  3|  1|  3|  0|  0|  3|  0|  0|  0|\n|blk_5647760196018...|Success|null|  0|  3|  1| 15|  3|  0|  0|  0|  3|  0|  3|  0|  0|  0|  0|  0|  0|  0|  0|  0|  3|  1|  3|  0|  0|  3|  0|  0|  0|\n|blk_-202775138379...|Success|null|  0|  3|  1| 15|  3|  0|  0|  0|  3|  0|  3|  0|  0|  0|  0|  0|  0|  0|  0|  0|  3|  1|  3|  0|  0|  3|  0|  0|  0|\n+--------------------+-------+----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d83ed6cd-a403-46e4-8a5b-51dfc75f7263",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[11]: 575061"
     ]
    }
   ],
   "source": [
    "df2.select(\"BlockId\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb4a8b77-d330-4bd6-bc6b-b09a210f74d3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[12]: 'blk_-1608999687919862906'"
     ]
    }
   ],
   "source": [
    "df2.select(\"BlockId\").first()[\"BlockId\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57a914bf-d943-4d75-87cd-83da83bcc730",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[13]: Row(BlockId='blk_1717858812220360316', Label='Success', Type=None, E1='0', E2='3', E3='1', E4='15', E5='3', E6='0', E7='0', E8='0', E9='3', E10='0', E11='3', E12='0', E13='0', E14='0', E15='0', E16='0', E17='0', E18='0', E19='0', E20='0', E21='3', E22='1', E23='3', E24='0', E25='0', E26='3', E27='0', E28='0', E29='0')"
     ]
    }
   ],
   "source": [
    "df2.collect()[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1bbc32c-986c-4bb2-ac8a-d49346e6cda6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[14]: ['BlockId', 'Log_Type']"
     ]
    }
   ],
   "source": [
    "df1 = df1.withColumnRenamed(\"Label\", \"Log_Type\")\n",
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85119441-7a11-4475-a394-2eee9b2fdf05",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+--------------------+-------+----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n|             BlockId|Log_Type|             BlockId|  Label|Type| E1| E2| E3| E4| E5| E6| E7| E8| E9|E10|E11|E12|E13|E14|E15|E16|E17|E18|E19|E20|E21|E22|E23|E24|E25|E26|E27|E28|E29|\n+--------------------+--------+--------------------+-------+----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n|blk_-100004553717...|  Normal|blk_-100004553717...|Success|null|  0|  0|  0|  0|  3|  0|  0|  0|  3|  0|  3|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  3|  0|  0|  0|\n|blk_-100021874837...|  Normal|blk_-100021874837...|Success|null|  0|  0|  0|  0|  3|  0|  0|  0|  3|  0|  3|  0|  0|  0|  0|  0|  0|  0|  0|  0|  3|  1|  3|  0|  0|  3|  0|  0|  0|\n|blk_-100028489120...|  Normal|blk_-100028489120...|Success|null|  0|  1|  2|  1|  3|  0|  0|  0|  3|  0|  3|  0|  0|  0|  0|  0|  0|  0|  0|  0|  3|  1|  3|  0|  0|  3|  0|  0|  0|\n|blk_-100029668880...|  Normal|blk_-100029668880...|Success|null|  0|  0|  0|  0|  3|  0|  0|  0|  3|  0|  3|  0|  0|  0|  0|  0|  0|  0|  0|  0|  3|  1|  3|  0|  0|  3|  0|  0|  0|\n|blk_-100058394360...|  Normal|blk_-100058394360...|Success|null|  0|  0|  0|  0|  3|  0|  0|  0|  3|  0|  3|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  3|  0|  0|  0|\n|blk_-100080480388...|  Normal|blk_-100080480388...|Success|null|  0|  0|  0|  0|  3|  0|  0|  0|  3|  0|  3|  0|  0|  0|  0|  0|  0|  0|  0|  0|  3|  1|  3|  0|  0|  3|  0|  0|  0|\n|blk_-100088503271...|  Normal|blk_-100088503271...|Success|null|  0|  0|  0|  0|  3|  0|  0|  0|  3|  0|  3|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  3|  0|  0|  0|\n|blk_-100097373580...|  Normal|blk_-100097373580...|Success|null|  0|  0|  2|  1|  3|  0|  0|  0|  3|  0|  3|  0|  0|  0|  0|  0|  0|  0|  0|  0|  3|  1|  3|  0|  0|  3|  0|  0|  0|\n|blk_-100113813561...|  Normal|blk_-100113813561...|Success|null|  0|  0|  8|  4|  3|  0|  0|  0|  3|  0|  3|  0|  0|  0|  0|  0|  0|  0|  0|  0|  3|  1|  3|  0|  0|  3|  0|  0|  0|\n|blk_-100121484143...|  Normal|blk_-100121484143...|Success|null|  0|  0|  0|  0|  3|  0|  0|  0|  3|  0|  3|  0|  0|  0|  0|  0|  0|  0|  0|  0|  3|  1|  3|  0|  0|  3|  0|  0|  0|\n|blk_-100138586392...|  Normal|blk_-100138586392...|Success|null|  0|  0|  0|  0|  3|  0|  0|  0|  3|  0|  3|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  3|  0|  0|  0|\n|blk_-100150424133...|  Normal|blk_-100150424133...|Success|null|  0|  0|  0|  0|  3|  0|  0|  0|  3|  0|  3|  0|  0|  0|  0|  0|  0|  0|  0|  0|  3|  1|  3|  0|  0|  3|  0|  0|  0|\n|blk_-100176349434...|  Normal|blk_-100176349434...|Success|null|  0|  1|  2|  1|  3|  0|  0|  0|  3|  0|  3|  0|  0|  0|  0|  0|  0|  0|  0|  0|  3|  1|  3|  0|  0|  3|  0|  0|  0|\n|blk_-100190044899...|  Normal|blk_-100190044899...|Success|null|  0|  0|  0|  0|  3|  0|  0|  0|  3|  0|  3|  0|  0|  0|  0|  0|  0|  0|  0|  0|  3|  1|  3|  0|  0|  3|  0|  0|  0|\n|blk_-100241217003...|  Normal|blk_-100241217003...|Success|null|  0|  0|  0|  0|  3|  0|  0|  0|  3|  0|  3|  0|  0|  0|  0|  0|  0|  0|  0|  0|  3|  1|  3|  0|  0|  3|  0|  0|  0|\n|blk_-100249716135...|  Normal|blk_-100249716135...|Success|null|  0|  0|  0|  0|  3|  0|  0|  0|  3|  0|  3|  0|  0|  0|  0|  0|  0|  0|  0|  0|  3|  1|  3|  0|  0|  3|  0|  0|  0|\n|blk_-100278736078...|  Normal|blk_-100278736078...|Success|null|  0|  1|  0|  0|  3|  0|  0|  0|  3|  0|  3|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  3|  0|  0|  0|\n|blk_-100326217011...|  Normal|blk_-100326217011...|Success|null|  0|  1|  4|  2|  3|  0|  0|  0|  3|  0|  3|  0|  0|  0|  0|  0|  0|  0|  0|  0|  3|  1|  3|  0|  0|  3|  0|  0|  0|\n|blk_-100379125492...|  Normal|blk_-100379125492...|Success|null|  0|  0|  0|  0|  3|  0|  0|  0|  3|  0|  3|  0|  0|  0|  0|  0|  0|  0|  0|  0|  3|  1|  3|  0|  0|  3|  0|  0|  0|\n|blk_-100419945855...|  Normal|blk_-100419945855...|Success|null|  0|  0|  0|  0|  3|  0|  0|  0|  3|  0|  3|  0|  0|  0|  0|  0|  0|  0|  0|  0|  3|  1|  3|  0|  0|  3|  0|  0|  0|\n+--------------------+--------+--------------------+-------+----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "merged = df1.join(df2, df1.BlockId == df2.BlockId, \"inner\")\n",
    "merged.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fb75356-3c7d-49e4-84e3-ee901d86c9e4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[18]: [('BlockId', 'string'),\n ('Log_Type', 'string'),\n ('BlockId', 'string'),\n ('Label', 'string'),\n ('Type', 'string'),\n ('E1', 'string'),\n ('E2', 'string'),\n ('E3', 'string'),\n ('E4', 'string'),\n ('E5', 'string'),\n ('E6', 'string'),\n ('E7', 'string'),\n ('E8', 'string'),\n ('E9', 'string'),\n ('E10', 'string'),\n ('E11', 'string'),\n ('E12', 'string'),\n ('E13', 'string'),\n ('E14', 'string'),\n ('E15', 'string'),\n ('E16', 'string'),\n ('E17', 'string'),\n ('E18', 'string'),\n ('E19', 'string'),\n ('E20', 'string'),\n ('E21', 'string'),\n ('E22', 'string'),\n ('E23', 'string'),\n ('E24', 'string'),\n ('E25', 'string'),\n ('E26', 'string'),\n ('E27', 'string'),\n ('E28', 'string'),\n ('E29', 'string')]"
     ]
    }
   ],
   "source": [
    "merged.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a39ce53a-9913-4f54-b7ee-c6137dc3bcaa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- BlockId: string (nullable = true)\n |-- Log_Type: string (nullable = true)\n |-- BlockId: string (nullable = true)\n |-- Label: string (nullable = true)\n |-- Type: string (nullable = true)\n |-- E1: integer (nullable = true)\n |-- E2: integer (nullable = true)\n |-- E3: integer (nullable = true)\n |-- E4: integer (nullable = true)\n |-- E5: integer (nullable = true)\n |-- E6: integer (nullable = true)\n |-- E7: integer (nullable = true)\n |-- E8: integer (nullable = true)\n |-- E9: integer (nullable = true)\n |-- E10: integer (nullable = true)\n |-- E11: integer (nullable = true)\n |-- E12: integer (nullable = true)\n |-- E13: integer (nullable = true)\n |-- E14: integer (nullable = true)\n |-- E15: integer (nullable = true)\n |-- E16: integer (nullable = true)\n |-- E17: integer (nullable = true)\n |-- E18: integer (nullable = true)\n |-- E19: integer (nullable = true)\n |-- E20: integer (nullable = true)\n |-- E21: integer (nullable = true)\n |-- E22: integer (nullable = true)\n |-- E23: integer (nullable = true)\n |-- E24: integer (nullable = true)\n |-- E25: integer (nullable = true)\n |-- E26: integer (nullable = true)\n |-- E27: integer (nullable = true)\n |-- E28: integer (nullable = true)\n |-- E29: integer (nullable = true)\n\n+--------------------+--------+--------------------+-------+----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n|             BlockId|Log_Type|             BlockId|  Label|Type| E1| E2| E3| E4| E5| E6| E7| E8| E9|E10|E11|E12|E13|E14|E15|E16|E17|E18|E19|E20|E21|E22|E23|E24|E25|E26|E27|E28|E29|\n+--------------------+--------+--------------------+-------+----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n|blk_-100004553717...|  Normal|blk_-100004553717...|Success|null|  0|  0|  0|  0|  3|  0|  0|  0|  3|  0|  3|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  3|  0|  0|  0|\n|blk_-100021874837...|  Normal|blk_-100021874837...|Success|null|  0|  0|  0|  0|  3|  0|  0|  0|  3|  0|  3|  0|  0|  0|  0|  0|  0|  0|  0|  0|  3|  1|  3|  0|  0|  3|  0|  0|  0|\n|blk_-100028489120...|  Normal|blk_-100028489120...|Success|null|  0|  1|  2|  1|  3|  0|  0|  0|  3|  0|  3|  0|  0|  0|  0|  0|  0|  0|  0|  0|  3|  1|  3|  0|  0|  3|  0|  0|  0|\n|blk_-100029668880...|  Normal|blk_-100029668880...|Success|null|  0|  0|  0|  0|  3|  0|  0|  0|  3|  0|  3|  0|  0|  0|  0|  0|  0|  0|  0|  0|  3|  1|  3|  0|  0|  3|  0|  0|  0|\n|blk_-100058394360...|  Normal|blk_-100058394360...|Success|null|  0|  0|  0|  0|  3|  0|  0|  0|  3|  0|  3|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  3|  0|  0|  0|\n|blk_-100080480388...|  Normal|blk_-100080480388...|Success|null|  0|  0|  0|  0|  3|  0|  0|  0|  3|  0|  3|  0|  0|  0|  0|  0|  0|  0|  0|  0|  3|  1|  3|  0|  0|  3|  0|  0|  0|\n|blk_-100088503271...|  Normal|blk_-100088503271...|Success|null|  0|  0|  0|  0|  3|  0|  0|  0|  3|  0|  3|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  3|  0|  0|  0|\n|blk_-100097373580...|  Normal|blk_-100097373580...|Success|null|  0|  0|  2|  1|  3|  0|  0|  0|  3|  0|  3|  0|  0|  0|  0|  0|  0|  0|  0|  0|  3|  1|  3|  0|  0|  3|  0|  0|  0|\n|blk_-100113813561...|  Normal|blk_-100113813561...|Success|null|  0|  0|  8|  4|  3|  0|  0|  0|  3|  0|  3|  0|  0|  0|  0|  0|  0|  0|  0|  0|  3|  1|  3|  0|  0|  3|  0|  0|  0|\n|blk_-100121484143...|  Normal|blk_-100121484143...|Success|null|  0|  0|  0|  0|  3|  0|  0|  0|  3|  0|  3|  0|  0|  0|  0|  0|  0|  0|  0|  0|  3|  1|  3|  0|  0|  3|  0|  0|  0|\n|blk_-100138586392...|  Normal|blk_-100138586392...|Success|null|  0|  0|  0|  0|  3|  0|  0|  0|  3|  0|  3|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  3|  0|  0|  0|\n|blk_-100150424133...|  Normal|blk_-100150424133...|Success|null|  0|  0|  0|  0|  3|  0|  0|  0|  3|  0|  3|  0|  0|  0|  0|  0|  0|  0|  0|  0|  3|  1|  3|  0|  0|  3|  0|  0|  0|\n|blk_-100176349434...|  Normal|blk_-100176349434...|Success|null|  0|  1|  2|  1|  3|  0|  0|  0|  3|  0|  3|  0|  0|  0|  0|  0|  0|  0|  0|  0|  3|  1|  3|  0|  0|  3|  0|  0|  0|\n|blk_-100190044899...|  Normal|blk_-100190044899...|Success|null|  0|  0|  0|  0|  3|  0|  0|  0|  3|  0|  3|  0|  0|  0|  0|  0|  0|  0|  0|  0|  3|  1|  3|  0|  0|  3|  0|  0|  0|\n|blk_-100241217003...|  Normal|blk_-100241217003...|Success|null|  0|  0|  0|  0|  3|  0|  0|  0|  3|  0|  3|  0|  0|  0|  0|  0|  0|  0|  0|  0|  3|  1|  3|  0|  0|  3|  0|  0|  0|\n|blk_-100249716135...|  Normal|blk_-100249716135...|Success|null|  0|  0|  0|  0|  3|  0|  0|  0|  3|  0|  3|  0|  0|  0|  0|  0|  0|  0|  0|  0|  3|  1|  3|  0|  0|  3|  0|  0|  0|\n|blk_-100278736078...|  Normal|blk_-100278736078...|Success|null|  0|  1|  0|  0|  3|  0|  0|  0|  3|  0|  3|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  3|  0|  0|  0|\n|blk_-100326217011...|  Normal|blk_-100326217011...|Success|null|  0|  1|  4|  2|  3|  0|  0|  0|  3|  0|  3|  0|  0|  0|  0|  0|  0|  0|  0|  0|  3|  1|  3|  0|  0|  3|  0|  0|  0|\n|blk_-100379125492...|  Normal|blk_-100379125492...|Success|null|  0|  0|  0|  0|  3|  0|  0|  0|  3|  0|  3|  0|  0|  0|  0|  0|  0|  0|  0|  0|  3|  1|  3|  0|  0|  3|  0|  0|  0|\n|blk_-100419945855...|  Normal|blk_-100419945855...|Success|null|  0|  0|  0|  0|  3|  0|  0|  0|  3|  0|  3|  0|  0|  0|  0|  0|  0|  0|  0|  0|  3|  1|  3|  0|  0|  3|  0|  0|  0|\n+--------------------+--------+--------------------+-------+----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# convert the event types datatype to int\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "columns_to_convert = [\n",
    "    'E1', 'E2', 'E3', 'E4', 'E5', 'E6', 'E7', 'E8', 'E9', 'E10',\n",
    "    'E11', 'E12', 'E13', 'E14', 'E15', 'E16', 'E17', 'E18', 'E19', 'E20',\n",
    "    'E21', 'E22', 'E23', 'E24', 'E25', 'E26', 'E27', 'E28', 'E29'\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "for column in columns_to_convert:\n",
    "    merged = merged.withColumn(column, col(column).cast('int'))\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "merged.printSchema()\n",
    "merged.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91bd7417-9be1-4365-a3da-01cedbb3d95e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[26]: ['BlockId',\n 'Label',\n 'BlockId',\n 'Label',\n 'Type',\n 'E1',\n 'E2',\n 'E3',\n 'E4',\n 'E5',\n 'E6',\n 'E7',\n 'E8',\n 'E9',\n 'E10',\n 'E11',\n 'E12',\n 'E13',\n 'E14',\n 'E15',\n 'E16',\n 'E17',\n 'E18',\n 'E19',\n 'E20',\n 'E21',\n 'E22',\n 'E23',\n 'E24',\n 'E25',\n 'E26',\n 'E27',\n 'E28',\n 'E29']"
     ]
    }
   ],
   "source": [
    "merged.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "549142b2-1376-4fe9-8aa1-49187aef691b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3378, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<command-4330738738621498>\", line 20, in <module>\n    raise ValueError(\"No numeric columns found in the DataFrame.\")\nValueError: No numeric columns found in the DataFrame.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 1997, in showtraceback\n    stb = self.InteractiveTB.structured_traceback(\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1112, in structured_traceback\n    return FormattedTB.structured_traceback(\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1006, in structured_traceback\n    return VerboseTB.structured_traceback(\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 859, in structured_traceback\n    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 812, in format_exception_as_a_whole\n    frames.append(self.format_record(r))\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 730, in format_record\n    result += ''.join(_format_traceback_lines(frame_info.lines, Colors, self.has_colors, lvals))\n  File \"/databricks/python/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n    value = obj.__dict__[self.func.__name__] = self.func(obj)\n  File \"/databricks/python/lib/python3.9/site-packages/stack_data/core.py\", line 698, in lines\n    pieces = self.included_pieces\n  File \"/databricks/python/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n    value = obj.__dict__[self.func.__name__] = self.func(obj)\n  File \"/databricks/python/lib/python3.9/site-packages/stack_data/core.py\", line 649, in included_pieces\n    pos = scope_pieces.index(self.executing_piece)\n  File \"/databricks/python/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n    value = obj.__dict__[self.func.__name__] = self.func(obj)\n  File \"/databricks/python/lib/python3.9/site-packages/stack_data/core.py\", line 628, in executing_piece\n    return only(\n  File \"/databricks/python/lib/python3.9/site-packages/executing/executing.py\", line 164, in only\n    raise NotOneValueFound('Expected one value, found 0')\nexecuting.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "<span class='ansi-red-fg'>ValueError</span>: No numeric columns found in the DataFrame.",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "# numeric_cols = [col for col, dtype in merged.dtypes if dtype in ['int', 'double']]\n",
    "\n",
    "# assembler = VectorAssembler(inputCols=numeric_cols, outputCol=\"features\")\n",
    "# merged_vector = assembler.transform(merged).select(\"features\")\n",
    "\n",
    "# # Calculate the correlation matrix\n",
    "# correlation_matrix = Correlation.corr(merged_vector, \"features\").head()[0]\n",
    "\n",
    "# # Convert to dense matrix and display\n",
    "# correlation_matrix = correlation_matrix.toArray()\n",
    "# print(correlation_matrix)\n",
    "\n",
    "numeric_cols = [col for col, dtype in merged.dtypes if dtype in ['int', 'double']]\n",
    "\n",
    "# Check if numeric_cols is empty\n",
    "if not numeric_cols:\n",
    "    raise ValueError(\"No numeric columns found in the DataFrame.\")\n",
    "\n",
    "print(f\"Numeric columns found: {numeric_cols}\")\n",
    "\n",
    "# Assemble the numeric columns into a feature vector\n",
    "assembler = VectorAssembler(inputCols=numeric_cols, outputCol=\"features\")\n",
    "merged_vector = assembler.transform(merged).select(\"features\")\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix = Correlation.corr(merged_vector, \"features\").head()[0]\n",
    "\n",
    "# Convert to dense matrix and display\n",
    "correlation_matrix = correlation_matrix.toArray()\n",
    "print(correlation_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c404963b-e13b-4313-93b0-7dc703451bd1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[18]: 16838"
     ]
    }
   ],
   "source": [
    "anomaly_fail_df = merged.filter((merged.Log_Type == 'Anomaly')).distinct().count()\n",
    "anomaly_fail_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4de56175-a08d-4010-97da-f11145812d7b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "unique_blockid_count_df2 = df1.select(\"BlockId\").distinct().count()\n",
    "print(unique_blockid_count_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0071f403-28b1-4669-96e6-f035aa89fe48",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[14]: ['BlockId', 'Label', 'Type', 'Features', 'TimeInterval', 'Latency']"
     ]
    }
   ],
   "source": [
    "df4.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3defbb24-d3d5-4d6f-837f-8984f6d17aed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HDFS Log Templates DataFrame:\n+-------+--------------------------------------------------------------------------------+\n|EventId|EventTemplate                                                                   |\n+-------+--------------------------------------------------------------------------------+\n|E1     |[*]Adding an already existing block[*]                                          |\n|E2     |[*]Verification succeeded for[*]                                                |\n|E3     |[*]Served block[*]to[*]                                                         |\n|E4     |[*]Got exception while serving[*]to[*]                                          |\n|E5     |[*]Receiving block[*]src:[*]dest:[*]                                            |\n|E6     |[*]Received block[*]src:[*]dest:[*]of size[*]                                   |\n|E7     |[*]writeBlock[*]received exception[*]                                           |\n|E8     |[*]PacketResponder[*]for block[*]Interrupted[*]                                 |\n|E9     |[*]Received block[*]of size[*]from[*]                                           |\n|E10    |[*]PacketResponder[*]Exception[*]                                               |\n|E11    |[*]PacketResponder[*]for block[*]terminating[*]                                 |\n|E12    |[*]:Exception writing block[*]to mirror[*]                                      |\n|E13    |[*]Receiving empty packet for block[*]                                          |\n|E14    |[*]Exception in receiveBlock for block[*]                                       |\n|E15    |[*]Changing block file offset of block[*]from[*]to[*]meta file offset to[*]     |\n|E16    |[*]:Transmitted block[*]to[*]                                                   |\n|E17    |[*]:Failed to transfer[*]to[*]got[*]                                            |\n|E18    |[*]Starting thread to transfer block[*]to[*]                                    |\n|E19    |[*]Reopen Block[*]                                                              |\n|E20    |[*]Unexpected error trying to delete block[*]BlockInfo not found in volumeMap[*]|\n+-------+--------------------------------------------------------------------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "print(\"HDFS Log Templates DataFrame:\")\n",
    "df3.show(truncate= False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11f2beb2-b1ec-4009-8dea-44f90a288a18",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event Traces DataFrame:\n+--------------------+-------+----+--------------------+--------------------+-------+\n|             BlockId|  Label|Type|            Features|        TimeInterval|Latency|\n+--------------------+-------+----+--------------------+--------------------+-------+\n|blk_-160899968791...|Success|null|[E5,E22,E5,E5,E11...|[0.0, 1.0, 0.0, 0...|   3802|\n|blk_7503483334202...|Success|null|[E5,E5,E22,E5,E11...|[0.0, 0.0, 1.0, 0...|   3802|\n|blk_-354458337728...|   Fail|  21|[E5,E22,E5,E5,E11...|[0.0, 1.0, 0.0, 0...|   3797|\n|blk_-907399258668...|Success|null|[E5,E22,E5,E5,E11...|[0.0, 1.0, 0.0, 0...|  50448|\n|blk_7854771516489...|Success|null|[E5,E5,E22,E5,E11...|[0.0, 0.0, 1.0, 4...|  50583|\n|blk_1717858812220...|Success|null|[E5,E5,E22,E5,E11...|[0.0, 0.0, 11.0, ...|  50458|\n|blk_-251961732037...|Success|null|[E5,E22,E5,E5,E11...|[0.0, 1.0, 9.0, 4...|  50523|\n|blk_7063315473424...|Success|null|[E5,E5,E5,E22,E11...|[1.0, 0.0, 0.0, 5...|  50818|\n|blk_8586544123689...|Success|null|[E5,E5,E5,E22,E11...|[1.0, 0.0, 0.0, 3...|  50795|\n|blk_2765344736980...|Success|null|[E5,E5,E22,E5,E11...|[0.0, 0.0, 1.0, 2...|  50528|\n|blk_-290049055749...|Success|null|[E5,E5,E22,E5,E11...|[0.0, 0.0, 7.0, 4...|  50440|\n|blk_-502732577314...|Success|null|[E5,E5,E22,E5,E9,...|[0.0, 0.0, 3.0, 3...|  50431|\n|blk_4394112519745...|Success|null|[E5,E5,E22,E5,E11...|[0.0, 0.0, 7.0, 3...|  50529|\n|blk_3640100967125...|Success|null|[E5,E5,E22,E5,E11...|[0.0, 0.0, 1.0, 4...|  50419|\n|blk_-401156444932...|Success|null|[E5,E5,E22,E5,E11...|[0.0, 0.0, 1.0, 3...|  50495|\n|blk_-853131033556...|   Fail|   4|[E5,E22,E5,E5,E11...|[0.0, 2.0, 0.0, 5...|  51107|\n|blk_-340992364514...|Success|null|[E5,E22,E5,E5,E11...|[0.0, 1.0, 9.0, 5...|  50443|\n|blk_3974948352784...|Success|null|[E5,E5,E22,E5,E11...|[0.0, 0.0, 3.0, 6...|  50824|\n|blk_5647760196018...|Success|null|[E5,E5,E5,E22,E11...|[0.0, 0.0, 0.0, 3...|  50556|\n|blk_-202775138379...|Success|null|[E5,E22,E5,E5,E11...|[0.0, 1.0, 10.0, ...|  50523|\n+--------------------+-------+----+--------------------+--------------------+-------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "print(\"Event Traces DataFrame:\")\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f9f51a5-cf55-4ae1-8ccb-b616c302d254",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+-------+----+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------+\n|BlockId                 |Label  |Type|Features                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |TimeInterval                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |Latency|\n+------------------------+-------+----+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------+\n|blk_-1608999687919862906|Success|null|[E5,E22,E5,E5,E11,E11,E9,E9,E11,E9,E26,E26,E26,E6,E5,E16,E6,E5,E18,E25,E26,E26,E3,E25,E6,E6,E5,E5,E16,E18,E26,E26,E5,E6,E5,E16,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E18,E25,E6,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E26,E26,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E25,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E18,E6,E5,E3,E3,E3,E3,E3,E16,E3,E3,E3,E3,E26,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E3,E23,E23,E23,E23,E23,E23,E23,E23,E23,E23,E21,E21,E21,E21,E21,E21,E21,E21,E21,E21]|[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 1.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 3.0, 0.0, 1.0, 40.0, 3703.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 25.0, 1.0, 0.0, 0.0, 1.0]|3802   |\n+------------------------+-------+----+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "df4.filter(col(\"blockid\") == 'blk_-1608999687919862906').show( truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35920b24-1433-41a9-9367-759ea71d6f57",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "unique_blockid_count_df4 = df1.select(\"BlockId\").distinct().count()\n",
    "print(unique_blockid_count_df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7d1c9cc-9684-4e34-bac7-8dd00d35691c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n|                 _c0|\n+--------------------+\n|081109 203518 143...|\n|081109 203518 35 ...|\n|081109 203519 143...|\n|081109 203519 145...|\n|081109 203519 145...|\n|081109 203519 145...|\n|081109 203519 145...|\n|081109 203519 145...|\n|081109 203519 147...|\n|081109 203519 147...|\n|081109 203519 29 ...|\n|081109 203519 30 ...|\n|081109 203519 31 ...|\n|081109 203520 142...|\n|081109 203520 145...|\n|081109 203520 26 ...|\n|081109 203521 143...|\n|081109 203521 143...|\n|081109 203521 144...|\n|081109 203521 145...|\n+--------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "hdfs_logs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "374a73e0-7b19-436d-81e1-c69a9f264631",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['081109 203518 143 INFO dfs.DataNode$DataXceiver: Receiving block blk_-1608999687919862906 src: /10.250.19.102:54106 dest: /10.250.19.102:50010', '081109 203518 35 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /mnt/hadoop/mapred/system/job_200811092030_0001/job.jar. blk_-1608999687919862906', '081109 203519 143 INFO dfs.DataNode$DataXceiver: Receiving block blk_-1608999687919862906 src: /10.250.10.6:40524 dest: /10.250.10.6:50010', '081109 203519 145 INFO dfs.DataNode$DataXceiver: Receiving block blk_-1608999687919862906 src: /10.250.14.224:42420 dest: /10.250.14.224:50010', '081109 203519 145 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_-1608999687919862906 terminating', '081109 203519 145 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_-1608999687919862906 terminating', '081109 203519 145 INFO dfs.DataNode$PacketResponder: Received block blk_-1608999687919862906 of size 91178 from /10.250.10.6', '081109 203519 145 INFO dfs.DataNode$PacketResponder: Received block blk_-1608999687919862906 of size 91178 from /10.250.19.102', '081109 203519 147 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_-1608999687919862906 terminating', '081109 203519 147 INFO dfs.DataNode$PacketResponder: Received block blk_-1608999687919862906 of size 91178 from /10.250.14.224']\n"
     ]
    }
   ],
   "source": [
    "logs = hdfs_logs.rdd.map(lambda row: row[0]).take(10)\n",
    "print(logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dfc055ee-0896-48c4-b149-cd01d64adeec",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Extracting block id from logs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed79c859-9b14-47e8-a7a6-c6e82ebd32a1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------+\n|_c0                                                                                                                                                                |blockid                 |\n+-------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------+\n|081109 203518 143 INFO dfs.DataNode$DataXceiver: Receiving block blk_-1608999687919862906 src: /10.250.19.102:54106 dest: /10.250.19.102:50010                     |blk_-1608999687919862906|\n|081109 203518 35 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /mnt/hadoop/mapred/system/job_200811092030_0001/job.jar. blk_-1608999687919862906         |blk_-1608999687919862906|\n|081109 203519 143 INFO dfs.DataNode$DataXceiver: Receiving block blk_-1608999687919862906 src: /10.250.10.6:40524 dest: /10.250.10.6:50010                         |blk_-1608999687919862906|\n|081109 203519 145 INFO dfs.DataNode$DataXceiver: Receiving block blk_-1608999687919862906 src: /10.250.14.224:42420 dest: /10.250.14.224:50010                     |blk_-1608999687919862906|\n|081109 203519 145 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_-1608999687919862906 terminating                                              |blk_-1608999687919862906|\n|081109 203519 145 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_-1608999687919862906 terminating                                              |blk_-1608999687919862906|\n|081109 203519 145 INFO dfs.DataNode$PacketResponder: Received block blk_-1608999687919862906 of size 91178 from /10.250.10.6                                       |blk_-1608999687919862906|\n|081109 203519 145 INFO dfs.DataNode$PacketResponder: Received block blk_-1608999687919862906 of size 91178 from /10.250.19.102                                     |blk_-1608999687919862906|\n|081109 203519 147 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_-1608999687919862906 terminating                                              |blk_-1608999687919862906|\n|081109 203519 147 INFO dfs.DataNode$PacketResponder: Received block blk_-1608999687919862906 of size 91178 from /10.250.14.224                                     |blk_-1608999687919862906|\n|081109 203519 29 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.10.6:50010 is added to blk_-1608999687919862906 size 91178      |blk_-1608999687919862906|\n|081109 203519 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.111.209:50010 is added to blk_-1608999687919862906 size 91178   |blk_-1608999687919862906|\n|081109 203519 31 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.14.224:50010 is added to blk_-1608999687919862906 size 91178    |blk_-1608999687919862906|\n|081109 203520 142 INFO dfs.DataNode$DataXceiver: Receiving block blk_7503483334202473044 src: /10.251.215.16:55695 dest: /10.251.215.16:50010                      |blk_7503483334202473044 |\n|081109 203520 145 INFO dfs.DataNode$DataXceiver: Receiving block blk_7503483334202473044 src: /10.250.19.102:34232 dest: /10.250.19.102:50010                      |blk_7503483334202473044 |\n|081109 203520 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /mnt/hadoop/mapred/system/job_200811092030_0001/job.split. blk_7503483334202473044        |blk_7503483334202473044 |\n|081109 203521 143 INFO dfs.DataNode$DataXceiver: Received block blk_-1608999687919862906 src: /10.251.215.16:52002 dest: /10.251.215.16:50010 of size 91178        |blk_-1608999687919862906|\n|081109 203521 143 INFO dfs.DataNode$DataXceiver: Receiving block blk_-1608999687919862906 src: /10.251.215.16:52002 dest: /10.251.215.16:50010                     |blk_-1608999687919862906|\n|081109 203521 144 INFO dfs.DataNode$DataXceiver: Receiving block blk_7503483334202473044 src: /10.251.71.16:51590 dest: /10.251.71.16:50010                        |blk_7503483334202473044 |\n|081109 203521 145 INFO dfs.DataNode$DataXceiver: Receiving block blk_-3544583377289625738 src: /10.250.19.102:39325 dest: /10.250.19.102:50010                     |blk_-3544583377289625738|\n|081109 203521 145 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_7503483334202473044 terminating                                               |blk_7503483334202473044 |\n|081109 203521 145 INFO dfs.DataNode$PacketResponder: Received block blk_7503483334202473044 of size 233217 from /10.251.215.16                                     |blk_7503483334202473044 |\n|081109 203521 146 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_7503483334202473044 terminating                                               |blk_7503483334202473044 |\n|081109 203521 146 INFO dfs.DataNode$PacketResponder: Received block blk_7503483334202473044 of size 233217 from /10.251.71.16                                      |blk_7503483334202473044 |\n|081109 203521 147 INFO dfs.DataNode$DataTransfer: 10.250.14.224:50010:Transmitted block blk_-1608999687919862906 to /10.251.215.16:50010                           |blk_-1608999687919862906|\n|081109 203521 147 INFO dfs.DataNode$DataXceiver: Received block blk_-1608999687919862906 src: /10.250.14.224:35754 dest: /10.250.14.224:50010 of size 91178        |blk_-1608999687919862906|\n|081109 203521 147 INFO dfs.DataNode$DataXceiver: Receiving block blk_-1608999687919862906 src: /10.250.14.224:35754 dest: /10.250.14.224:50010                     |blk_-1608999687919862906|\n|081109 203521 148 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_7503483334202473044 terminating                                               |blk_7503483334202473044 |\n|081109 203521 148 INFO dfs.DataNode$PacketResponder: Received block blk_7503483334202473044 of size 233217 from /10.250.19.102                                     |blk_7503483334202473044 |\n|081109 203521 19 INFO dfs.DataNode: 10.250.14.224:50010 Starting thread to transfer block blk_-1608999687919862906 to 10.251.215.16:50010                          |blk_-1608999687919862906|\n|081109 203521 19 INFO dfs.FSNamesystem: BLOCK* ask 10.250.14.224:50010 to replicate blk_-1608999687919862906 to datanode(s) 10.251.215.16:50010 10.251.71.193:50010|blk_-1608999687919862906|\n|081109 203521 27 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.106.10:50010 is added to blk_7503483334202473044 size 233217    |blk_7503483334202473044 |\n|081109 203521 29 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /mnt/hadoop/mapred/system/job_200811092030_0001/job.xml. blk_-3544583377289625738         |blk_-3544583377289625738|\n|081109 203521 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.71.193:50010 is added to blk_-1608999687919862906 size 91178    |blk_-1608999687919862906|\n|081109 203521 33 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.215.16:50010 is added to blk_7503483334202473044 size 233217    |blk_7503483334202473044 |\n|081109 203521 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.215.16:50010 is added to blk_-1608999687919862906 size 91178    |blk_-1608999687919862906|\n|081109 203521 35 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.71.16:50010 is added to blk_7503483334202473044 size 233217     |blk_7503483334202473044 |\n|081109 203522 144 INFO dfs.DataNode$DataXceiver: Receiving block blk_-3544583377289625738 src: /10.251.197.226:60229 dest: /10.251.197.226:50010                   |blk_-3544583377289625738|\n|081109 203522 145 INFO dfs.DataNode$DataXceiver: Receiving block blk_-3544583377289625738 src: /10.250.11.100:54800 dest: /10.250.11.100:50010                     |blk_-3544583377289625738|\n|081109 203522 147 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_-3544583377289625738 terminating                                              |blk_-3544583377289625738|\n+-------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------+\nonly showing top 40 rows\n\n"
     ]
    }
   ],
   "source": [
    "regex_pattern = r'(blk_-?\\d+)'\n",
    "\n",
    "data_with_blockid = hdfs_logs.withColumn(\"blockid\", regexp_extract(hdfs_logs['_c0'], regex_pattern, 1))\n",
    "    \n",
    "data_with_blockid.show(40, truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc459c24-b8d4-493f-8426-67894321d0c7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, collect_list, udf, array_contains\n",
    "from pyspark.sql.types import StringType, ArrayType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab32288a-23b1-421d-8892-27672211b176",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------+\n|_c0                                                                                                                                                             |blockid                 |\n+----------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------+\n|081109 203518 143 INFO dfs.DataNode$DataXceiver: Receiving block blk_-1608999687919862906 src: /10.250.19.102:54106 dest: /10.250.19.102:50010                  |blk_-1608999687919862906|\n|081109 203518 35 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /mnt/hadoop/mapred/system/job_200811092030_0001/job.jar. blk_-1608999687919862906      |blk_-1608999687919862906|\n|081109 203519 143 INFO dfs.DataNode$DataXceiver: Receiving block blk_-1608999687919862906 src: /10.250.10.6:40524 dest: /10.250.10.6:50010                      |blk_-1608999687919862906|\n|081109 203519 145 INFO dfs.DataNode$DataXceiver: Receiving block blk_-1608999687919862906 src: /10.250.14.224:42420 dest: /10.250.14.224:50010                  |blk_-1608999687919862906|\n|081109 203519 145 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_-1608999687919862906 terminating                                           |blk_-1608999687919862906|\n|081109 203519 145 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_-1608999687919862906 terminating                                           |blk_-1608999687919862906|\n|081109 203519 145 INFO dfs.DataNode$PacketResponder: Received block blk_-1608999687919862906 of size 91178 from /10.250.10.6                                    |blk_-1608999687919862906|\n|081109 203519 145 INFO dfs.DataNode$PacketResponder: Received block blk_-1608999687919862906 of size 91178 from /10.250.19.102                                  |blk_-1608999687919862906|\n|081109 203519 147 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_-1608999687919862906 terminating                                           |blk_-1608999687919862906|\n|081109 203519 147 INFO dfs.DataNode$PacketResponder: Received block blk_-1608999687919862906 of size 91178 from /10.250.14.224                                  |blk_-1608999687919862906|\n|081109 203519 29 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.10.6:50010 is added to blk_-1608999687919862906 size 91178   |blk_-1608999687919862906|\n|081109 203519 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.111.209:50010 is added to blk_-1608999687919862906 size 91178|blk_-1608999687919862906|\n|081109 203519 31 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.14.224:50010 is added to blk_-1608999687919862906 size 91178 |blk_-1608999687919862906|\n|081109 203521 143 INFO dfs.DataNode$DataXceiver: Received block blk_-1608999687919862906 src: /10.251.215.16:52002 dest: /10.251.215.16:50010 of size 91178     |blk_-1608999687919862906|\n|081109 203521 143 INFO dfs.DataNode$DataXceiver: Receiving block blk_-1608999687919862906 src: /10.251.215.16:52002 dest: /10.251.215.16:50010                  |blk_-1608999687919862906|\n|081109 203521 145 INFO dfs.DataNode$DataXceiver: Receiving block blk_-3544583377289625738 src: /10.250.19.102:39325 dest: /10.250.19.102:50010                  |blk_-3544583377289625738|\n|081109 203521 147 INFO dfs.DataNode$DataTransfer: 10.250.14.224:50010:Transmitted block blk_-1608999687919862906 to /10.251.215.16:50010                        |blk_-1608999687919862906|\n|081109 203521 147 INFO dfs.DataNode$DataXceiver: Received block blk_-1608999687919862906 src: /10.250.14.224:35754 dest: /10.250.14.224:50010 of size 91178     |blk_-1608999687919862906|\n|081109 203521 147 INFO dfs.DataNode$DataXceiver: Receiving block blk_-1608999687919862906 src: /10.250.14.224:35754 dest: /10.250.14.224:50010                  |blk_-1608999687919862906|\n|081109 203521 19 INFO dfs.DataNode: 10.250.14.224:50010 Starting thread to transfer block blk_-1608999687919862906 to 10.251.215.16:50010                       |blk_-1608999687919862906|\n+----------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "demo_raw = data_with_blockid.filter((col(\"blockid\") == 'blk_-1608999687919862906') | (col(\"blockid\") == 'blk_-3544583377289625738'))\n",
    "demo_raw.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a9239e8-0a28-4301-9c00-f8c39790d8d3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "output_path = \"dbfs:/FileStore/shared_uploads/tanmay.k.singh@sjsu.edu/demo_raw.csv\"  # Replace with your desired output path\n",
    "data_with_blockid.write.csv(output_path, header=True, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20927406-54ab-44f1-9c78-f436f8e93cef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n|                 _c0|             blockid|\n+--------------------+--------------------+\n|081109 203518 143...|blk_-160899968791...|\n|081109 203518 35 ...|blk_-160899968791...|\n|081109 203519 143...|blk_-160899968791...|\n|081109 203519 145...|blk_-160899968791...|\n|081109 203519 145...|blk_-160899968791...|\n|081109 203519 145...|blk_-160899968791...|\n|081109 203519 145...|blk_-160899968791...|\n|081109 203519 145...|blk_-160899968791...|\n|081109 203519 147...|blk_-160899968791...|\n|081109 203519 147...|blk_-160899968791...|\n|081109 203519 29 ...|blk_-160899968791...|\n|081109 203519 30 ...|blk_-160899968791...|\n|081109 203519 31 ...|blk_-160899968791...|\n|081109 203521 143...|blk_-160899968791...|\n|081109 203521 143...|blk_-160899968791...|\n|081109 203521 145...|blk_-354458337728...|\n|081109 203521 147...|blk_-160899968791...|\n|081109 203521 147...|blk_-160899968791...|\n|081109 203521 147...|blk_-160899968791...|\n|081109 203521 19 ...|blk_-160899968791...|\n+--------------------+--------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "demo = spark.read.format(\"csv\").option(\"header\", \"true\").load(output_path)\n",
    "demo.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "697e95b5-b483-46e0-8f03-48553d8e1dfe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df3 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"dbfs:/FileStore/shared_uploads/tanmay.k.singh@sjsu.edu/HDFS_log_templates.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05ac9c46-ac06-400a-8d0a-77155327151a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def finding_events(logs_df):\n",
    "\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql.functions import col, collect_list, udf, flatten\n",
    "    from pyspark.sql.types import StringType, ArrayType\n",
    "    import re\n",
    "\n",
    "    # Initialize Spark session\n",
    "    spark = SparkSession.builder.appName(\"BlockID Events\").getOrCreate()\n",
    "\n",
    "    # events_df = spark.createDataFrame(event_data, event_columns)\n",
    "    events_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"dbfs:/FileStore/shared_uploads/tanmay.k.singh@sjsu.edu/HDFS_log_templates.csv\")\n",
    "\n",
    "    # Broadcast the event templates\n",
    "    event_templates = {row.EventId: row.EventTemplate for row in events_df.collect()}\n",
    "    event_templates_broadcast = spark.sparkContext.broadcast(event_templates)\n",
    "\n",
    "    # Define UDF to extract events\n",
    "    def extract_events(log_message):\n",
    "        matched_events = []\n",
    "        templates = event_templates_broadcast.value\n",
    "        for event_id, template in templates.items():\n",
    "            pattern = template.replace(\"[*]\", \".*\")\n",
    "            if re.search(pattern, log_message):\n",
    "                matched_events.append(event_id)\n",
    "        return matched_events\n",
    "\n",
    "    extract_events_udf = udf(extract_events, ArrayType(StringType()))\n",
    "\n",
    "    # Apply the UDF to extract events from log messages\n",
    "    logs_with_events_df = logs_df.withColumn(\"events\", extract_events_udf(col(\"_c0\")))\n",
    "\n",
    "    # Show intermediate DataFrame\n",
    "    logs_with_events_df.show(truncate=False)\n",
    "\n",
    "    # Group by blockid and aggregate the events\n",
    "    aggregated_df = logs_with_events_df.groupBy(\"blockid\").agg(collect_list(\"events\").alias(\"event_lists\"))\n",
    "\n",
    "    # Flatten the nested lists\n",
    "    flattened_df = aggregated_df.withColumn(\"event_list\", flatten(col(\"event_lists\"))).drop(\"event_lists\")\n",
    "\n",
    "    # Show final DataFrame\n",
    "    flattened_df.show(truncate=False)\n",
    "    return flattened_df\n",
    "# # Stop the Spark session\n",
    "# spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7699d6f-93db-407c-83d0-e0ba1fc9b030",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------+------+\n|_c0                                                                                                                                                             |blockid                 |events|\n+----------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------+------+\n|081109 203518 143 INFO dfs.DataNode$DataXceiver: Receiving block blk_-1608999687919862906 src: /10.250.19.102:54106 dest: /10.250.19.102:50010                  |blk_-1608999687919862906|[E5]  |\n|081109 203518 35 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /mnt/hadoop/mapred/system/job_200811092030_0001/job.jar. blk_-1608999687919862906      |blk_-1608999687919862906|[]    |\n|081109 203519 143 INFO dfs.DataNode$DataXceiver: Receiving block blk_-1608999687919862906 src: /10.250.10.6:40524 dest: /10.250.10.6:50010                      |blk_-1608999687919862906|[E5]  |\n|081109 203519 145 INFO dfs.DataNode$DataXceiver: Receiving block blk_-1608999687919862906 src: /10.250.14.224:42420 dest: /10.250.14.224:50010                  |blk_-1608999687919862906|[E5]  |\n|081109 203519 145 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_-1608999687919862906 terminating                                           |blk_-1608999687919862906|[E11] |\n|081109 203519 145 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_-1608999687919862906 terminating                                           |blk_-1608999687919862906|[E11] |\n|081109 203519 145 INFO dfs.DataNode$PacketResponder: Received block blk_-1608999687919862906 of size 91178 from /10.250.10.6                                    |blk_-1608999687919862906|[E9]  |\n|081109 203519 145 INFO dfs.DataNode$PacketResponder: Received block blk_-1608999687919862906 of size 91178 from /10.250.19.102                                  |blk_-1608999687919862906|[E9]  |\n|081109 203519 147 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_-1608999687919862906 terminating                                           |blk_-1608999687919862906|[E11] |\n|081109 203519 147 INFO dfs.DataNode$PacketResponder: Received block blk_-1608999687919862906 of size 91178 from /10.250.14.224                                  |blk_-1608999687919862906|[E9]  |\n|081109 203519 29 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.10.6:50010 is added to blk_-1608999687919862906 size 91178   |blk_-1608999687919862906|[]    |\n|081109 203519 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.111.209:50010 is added to blk_-1608999687919862906 size 91178|blk_-1608999687919862906|[]    |\n|081109 203519 31 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.14.224:50010 is added to blk_-1608999687919862906 size 91178 |blk_-1608999687919862906|[]    |\n|081109 203521 143 INFO dfs.DataNode$DataXceiver: Received block blk_-1608999687919862906 src: /10.251.215.16:52002 dest: /10.251.215.16:50010 of size 91178     |blk_-1608999687919862906|[E6]  |\n|081109 203521 143 INFO dfs.DataNode$DataXceiver: Receiving block blk_-1608999687919862906 src: /10.251.215.16:52002 dest: /10.251.215.16:50010                  |blk_-1608999687919862906|[E5]  |\n|081109 203521 145 INFO dfs.DataNode$DataXceiver: Receiving block blk_-3544583377289625738 src: /10.250.19.102:39325 dest: /10.250.19.102:50010                  |blk_-3544583377289625738|[E5]  |\n|081109 203521 147 INFO dfs.DataNode$DataTransfer: 10.250.14.224:50010:Transmitted block blk_-1608999687919862906 to /10.251.215.16:50010                        |blk_-1608999687919862906|[E16] |\n|081109 203521 147 INFO dfs.DataNode$DataXceiver: Received block blk_-1608999687919862906 src: /10.250.14.224:35754 dest: /10.250.14.224:50010 of size 91178     |blk_-1608999687919862906|[E6]  |\n|081109 203521 147 INFO dfs.DataNode$DataXceiver: Receiving block blk_-1608999687919862906 src: /10.250.14.224:35754 dest: /10.250.14.224:50010                  |blk_-1608999687919862906|[E5]  |\n|081109 203521 19 INFO dfs.DataNode: 10.250.14.224:50010 Starting thread to transfer block blk_-1608999687919862906 to 10.251.215.16:50010                       |blk_-1608999687919862906|[E18] |\n+----------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------+------+\nonly showing top 20 rows\n\n+------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|blockid                 |event_list                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n+------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|blk_-1608999687919862906|[E5, E5, E5, E11, E11, E9, E9, E11, E9, E6, E5, E16, E6, E5, E18, E3, E6, E6, E5, E5, E16, E18, E5, E6, E5, E16, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E18, E6, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E18, E6, E5, E3, E3, E3, E3, E3, E16, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E21, E21, E21, E21, E21, E21, E21, E21, E21, E21]|\n|blk_-3544583377289625738|[E5, E5, E5, E11, E9, E11, E9, E11, E9, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E3, E21, E21, E21, E20]                                                                                                                              |\n+------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "demo = finding_events(demo_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f89d720d-5fca-4f5d-9b62-f60b017bfa56",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n|             blockid|          event_list|\n+--------------------+--------------------+\n|blk_-160899968791...|[E5, E5, E5, E11,...|\n|blk_-354458337728...|[E5, E5, E5, E11,...|\n+--------------------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "demo.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41070f70-e997-476f-800c-20a20ecd3e37",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def  convert_to_matrix (df):\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql.functions import col, collect_list, flatten, udf, coalesce, lit\n",
    "    from pyspark.sql.types import StringType, IntegerType, MapType\n",
    "\n",
    "    # Initialize Spark session\n",
    "    spark = SparkSession.builder.appName(\"BlockID Events Transformation\").getOrCreate()\n",
    "    # Define UDF to count events\n",
    "    def count_events(event_list):\n",
    "        event_counts = {}\n",
    "        for event in event_list:\n",
    "            if event in event_counts:\n",
    "                event_counts[event] += 1\n",
    "            else:\n",
    "                event_counts[event] = 1\n",
    "        return event_counts\n",
    "\n",
    "    count_events_udf = udf(count_events, MapType(StringType(), IntegerType()))\n",
    "\n",
    "    # Apply the UDF to add a new column with event counts\n",
    "    df_with_counts = df.withColumn(\"event_counts\", count_events_udf(col(\"event_list\")))\n",
    "    df_with_counts.show(truncate=False)\n",
    "\n",
    "    # Get a list of all possible events (you can extract this from the events DataFrame)\n",
    "    all_events = [\"E1\", \"E2\", \"E3\", \"E4\", \"E5\", \"E6\", \"E7\", \"E8\", \"E9\", \"E10\", \"E11\", \"E12\", \"E13\", \"E14\", \"E15\", \"E16\", \"E17\", \"E18\", \"E19\", \"E20\", \"E21\", \"E22\", \"E23\", \"E24\", \"E25\", \"E26\", \"E27\", \"E28\", \"E29\"]\n",
    "\n",
    "    # Create columns for each event with count values\n",
    "    for event in all_events:\n",
    "        df_with_counts = df_with_counts.withColumn(event, coalesce(col(\"event_counts\").getItem(event), lit(0)))\n",
    "\n",
    "    # Drop the event_counts column\n",
    "    final_df = df_with_counts.drop(\"event_counts\")\n",
    "\n",
    "    # Show the final DataFrame\n",
    "    final_df.show(truncate=False)\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69e68e09-c24e-4d93-9255-434f0427adc7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def combine_events()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, collect_list, udf, flatten\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "import re\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"BlockID Events\").getOrCreate()\n",
    "event_columns = [\"EventId\", \"EventTemplate\"]\n",
    "\n",
    "# logs_df = spark.createDataFrame(log_data, log_columns)\n",
    "logs_df = data_with_blockid\n",
    "# events_df = spark.createDataFrame(event_data, event_columns)\n",
    "events_df = df3\n",
    "\n",
    "# Show DataFrames\n",
    "logs_df.show(truncate=False)\n",
    "events_df.show(truncate=False)\n",
    "\n",
    "# Broadcast the event templates\n",
    "event_templates = {row.EventId: row.EventTemplate for row in events_df.collect()}\n",
    "event_templates_broadcast = spark.sparkContext.broadcast(event_templates)\n",
    "\n",
    "# Define UDF to extract events\n",
    "def extract_events(log_message):\n",
    "    matched_events = []\n",
    "    templates = event_templates_broadcast.value\n",
    "    for event_id, template in templates.items():\n",
    "        pattern = template.replace(\"[*]\", \".*\")\n",
    "        if re.search(pattern, log_message):\n",
    "            matched_events.append(event_id)\n",
    "    return matched_events\n",
    "\n",
    "extract_events_udf = udf(extract_events, ArrayType(StringType()))\n",
    "\n",
    "# Apply the UDF to extract events from log messages\n",
    "logs_with_events_df = logs_df.withColumn(\"events\", extract_events_udf(col(\"_c0\")))\n",
    "\n",
    "# Show intermediate DataFrame\n",
    "logs_with_events_df.show(truncate=False)\n",
    "\n",
    "# Group by blockid and aggregate the events\n",
    "aggregated_df = logs_with_events_df.groupBy(\"blockid\").agg(collect_list(\"events\").alias(\"event_lists\"))\n",
    "\n",
    "# Flatten the nested lists\n",
    "flattened_df = aggregated_df.withColumn(\"event_list\", flatten(col(\"event_lists\"))).drop(\"event_lists\")\n",
    "\n",
    "# Show final DataFrame\n",
    "flattened_df.show(truncate=False)\n",
    "\n",
    "# # Stop the Spark session\n",
    "# spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a5ac4ce-86eb-43f0-83d0-715878530043",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# extract_events_udf = udf(df3, ArrayType(StringType()))\n",
    "# logs_with_events_df = extract_events_udf.withColumn(\"events\", extract_events_udf(col(\"log_message\")))\n",
    "aggregated_df = data_with_blockid.groupBy(\"blockid\").agg(collect_list(\"_c0\").alias(\"event_lists\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d99b7c26-adc3-4b5c-b722-631a836b31b1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Exctracting Events for each block id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fb47b74-ff14-4584-af17-ea6c32cc740b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------+\n|_c0                                                                                                                                                             |blockid                 |\n+----------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------+\n|081109 203518 143 INFO dfs.DataNode$DataXceiver: Receiving block blk_-1608999687919862906 src: /10.250.19.102:54106 dest: /10.250.19.102:50010                  |blk_-1608999687919862906|\n|081109 203518 35 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /mnt/hadoop/mapred/system/job_200811092030_0001/job.jar. blk_-1608999687919862906      |blk_-1608999687919862906|\n|081109 203519 143 INFO dfs.DataNode$DataXceiver: Receiving block blk_-1608999687919862906 src: /10.250.10.6:40524 dest: /10.250.10.6:50010                      |blk_-1608999687919862906|\n|081109 203519 145 INFO dfs.DataNode$DataXceiver: Receiving block blk_-1608999687919862906 src: /10.250.14.224:42420 dest: /10.250.14.224:50010                  |blk_-1608999687919862906|\n|081109 203519 145 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_-1608999687919862906 terminating                                           |blk_-1608999687919862906|\n|081109 203519 145 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_-1608999687919862906 terminating                                           |blk_-1608999687919862906|\n|081109 203519 145 INFO dfs.DataNode$PacketResponder: Received block blk_-1608999687919862906 of size 91178 from /10.250.10.6                                    |blk_-1608999687919862906|\n|081109 203519 145 INFO dfs.DataNode$PacketResponder: Received block blk_-1608999687919862906 of size 91178 from /10.250.19.102                                  |blk_-1608999687919862906|\n|081109 203519 147 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_-1608999687919862906 terminating                                           |blk_-1608999687919862906|\n|081109 203519 147 INFO dfs.DataNode$PacketResponder: Received block blk_-1608999687919862906 of size 91178 from /10.250.14.224                                  |blk_-1608999687919862906|\n|081109 203519 29 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.10.6:50010 is added to blk_-1608999687919862906 size 91178   |blk_-1608999687919862906|\n|081109 203519 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.111.209:50010 is added to blk_-1608999687919862906 size 91178|blk_-1608999687919862906|\n|081109 203519 31 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.14.224:50010 is added to blk_-1608999687919862906 size 91178 |blk_-1608999687919862906|\n|081109 203520 142 INFO dfs.DataNode$DataXceiver: Receiving block blk_7503483334202473044 src: /10.251.215.16:55695 dest: /10.251.215.16:50010                   |blk_7503483334202473044 |\n|081109 203520 145 INFO dfs.DataNode$DataXceiver: Receiving block blk_7503483334202473044 src: /10.250.19.102:34232 dest: /10.250.19.102:50010                   |blk_7503483334202473044 |\n|081109 203520 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /mnt/hadoop/mapred/system/job_200811092030_0001/job.split. blk_7503483334202473044     |blk_7503483334202473044 |\n|081109 203521 143 INFO dfs.DataNode$DataXceiver: Received block blk_-1608999687919862906 src: /10.251.215.16:52002 dest: /10.251.215.16:50010 of size 91178     |blk_-1608999687919862906|\n|081109 203521 143 INFO dfs.DataNode$DataXceiver: Receiving block blk_-1608999687919862906 src: /10.251.215.16:52002 dest: /10.251.215.16:50010                  |blk_-1608999687919862906|\n|081109 203521 144 INFO dfs.DataNode$DataXceiver: Receiving block blk_7503483334202473044 src: /10.251.71.16:51590 dest: /10.251.71.16:50010                     |blk_7503483334202473044 |\n|081109 203521 145 INFO dfs.DataNode$DataXceiver: Receiving block blk_-3544583377289625738 src: /10.250.19.102:39325 dest: /10.250.19.102:50010                  |blk_-3544583377289625738|\n+----------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------+\nonly showing top 20 rows\n\n+-------+--------------------------------------------------------------------------------+\n|EventId|EventTemplate                                                                   |\n+-------+--------------------------------------------------------------------------------+\n|E1     |[*]Adding an already existing block[*]                                          |\n|E2     |[*]Verification succeeded for[*]                                                |\n|E3     |[*]Served block[*]to[*]                                                         |\n|E4     |[*]Got exception while serving[*]to[*]                                          |\n|E5     |[*]Receiving block[*]src:[*]dest:[*]                                            |\n|E6     |[*]Received block[*]src:[*]dest:[*]of size[*]                                   |\n|E7     |[*]writeBlock[*]received exception[*]                                           |\n|E8     |[*]PacketResponder[*]for block[*]Interrupted[*]                                 |\n|E9     |[*]Received block[*]of size[*]from[*]                                           |\n|E10    |[*]PacketResponder[*]Exception[*]                                               |\n|E11    |[*]PacketResponder[*]for block[*]terminating[*]                                 |\n|E12    |[*]:Exception writing block[*]to mirror[*]                                      |\n|E13    |[*]Receiving empty packet for block[*]                                          |\n|E14    |[*]Exception in receiveBlock for block[*]                                       |\n|E15    |[*]Changing block file offset of block[*]from[*]to[*]meta file offset to[*]     |\n|E16    |[*]:Transmitted block[*]to[*]                                                   |\n|E17    |[*]:Failed to transfer[*]to[*]got[*]                                            |\n|E18    |[*]Starting thread to transfer block[*]to[*]                                    |\n|E19    |[*]Reopen Block[*]                                                              |\n|E20    |[*]Unexpected error trying to delete block[*]BlockInfo not found in volumeMap[*]|\n+-------+--------------------------------------------------------------------------------+\nonly showing top 20 rows\n\n+----------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------+------+\n|_c0                                                                                                                                                             |blockid                 |events|\n+----------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------+------+\n|081109 203518 143 INFO dfs.DataNode$DataXceiver: Receiving block blk_-1608999687919862906 src: /10.250.19.102:54106 dest: /10.250.19.102:50010                  |blk_-1608999687919862906|[E5]  |\n|081109 203518 35 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /mnt/hadoop/mapred/system/job_200811092030_0001/job.jar. blk_-1608999687919862906      |blk_-1608999687919862906|[]    |\n|081109 203519 143 INFO dfs.DataNode$DataXceiver: Receiving block blk_-1608999687919862906 src: /10.250.10.6:40524 dest: /10.250.10.6:50010                      |blk_-1608999687919862906|[E5]  |\n|081109 203519 145 INFO dfs.DataNode$DataXceiver: Receiving block blk_-1608999687919862906 src: /10.250.14.224:42420 dest: /10.250.14.224:50010                  |blk_-1608999687919862906|[E5]  |\n|081109 203519 145 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_-1608999687919862906 terminating                                           |blk_-1608999687919862906|[E11] |\n|081109 203519 145 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_-1608999687919862906 terminating                                           |blk_-1608999687919862906|[E11] |\n|081109 203519 145 INFO dfs.DataNode$PacketResponder: Received block blk_-1608999687919862906 of size 91178 from /10.250.10.6                                    |blk_-1608999687919862906|[E9]  |\n|081109 203519 145 INFO dfs.DataNode$PacketResponder: Received block blk_-1608999687919862906 of size 91178 from /10.250.19.102                                  |blk_-1608999687919862906|[E9]  |\n|081109 203519 147 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_-1608999687919862906 terminating                                           |blk_-1608999687919862906|[E11] |\n|081109 203519 147 INFO dfs.DataNode$PacketResponder: Received block blk_-1608999687919862906 of size 91178 from /10.250.14.224                                  |blk_-1608999687919862906|[E9]  |\n|081109 203519 29 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.10.6:50010 is added to blk_-1608999687919862906 size 91178   |blk_-1608999687919862906|[]    |\n|081109 203519 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.111.209:50010 is added to blk_-1608999687919862906 size 91178|blk_-1608999687919862906|[]    |\n|081109 203519 31 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.14.224:50010 is added to blk_-1608999687919862906 size 91178 |blk_-1608999687919862906|[]    |\n|081109 203520 142 INFO dfs.DataNode$DataXceiver: Receiving block blk_7503483334202473044 src: /10.251.215.16:55695 dest: /10.251.215.16:50010                   |blk_7503483334202473044 |[E5]  |\n|081109 203520 145 INFO dfs.DataNode$DataXceiver: Receiving block blk_7503483334202473044 src: /10.250.19.102:34232 dest: /10.250.19.102:50010                   |blk_7503483334202473044 |[E5]  |\n|081109 203520 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /mnt/hadoop/mapred/system/job_200811092030_0001/job.split. blk_7503483334202473044     |blk_7503483334202473044 |[]    |\n|081109 203521 143 INFO dfs.DataNode$DataXceiver: Received block blk_-1608999687919862906 src: /10.251.215.16:52002 dest: /10.251.215.16:50010 of size 91178     |blk_-1608999687919862906|[E6]  |\n|081109 203521 143 INFO dfs.DataNode$DataXceiver: Receiving block blk_-1608999687919862906 src: /10.251.215.16:52002 dest: /10.251.215.16:50010                  |blk_-1608999687919862906|[E5]  |\n|081109 203521 144 INFO dfs.DataNode$DataXceiver: Receiving block blk_7503483334202473044 src: /10.251.71.16:51590 dest: /10.251.71.16:50010                     |blk_7503483334202473044 |[E5]  |\n|081109 203521 145 INFO dfs.DataNode$DataXceiver: Receiving block blk_-3544583377289625738 src: /10.250.19.102:39325 dest: /10.250.19.102:50010                  |blk_-3544583377289625738|[E5]  |\n+----------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------+------+\nonly showing top 20 rows\n\n+------------------------+------------------------------------------------------------------------------------------------------+\n|blockid                 |event_list                                                                                            |\n+------------------------+------------------------------------------------------------------------------------------------------+\n|blk_-100004553717737248 |[E5, E5, E5, E11, E9, E11, E9, E11, E9]                                                               |\n|blk_-1000218748372878626|[E5, E5, E5, E11, E9, E11, E9, E11, E9, E21, E21, E21]                                                |\n|blk_-1000284891202066880|[E5, E5, E5, E11, E9, E11, E9, E11, E9, E2, E3, E3, E4, E21, E21, E21]                                |\n|blk_-1000296688806487470|[E5, E5, E5, E11, E9, E11, E9, E11, E9, E21, E21, E21]                                                |\n|blk_-1000583943604118788|[E5, E5, E5, E11, E9, E11, E9, E11, E9]                                                               |\n|blk_-1000804803887048752|[E5, E5, E5, E11, E9, E11, E9, E11, E9, E21, E21, E21]                                                |\n|blk_-100088503271939182 |[E5, E5, E5, E11, E9, E11, E9, E11, E9]                                                               |\n|blk_-1000973735807259699|[E5, E5, E5, E11, E9, E11, E9, E11, E9, E3, E4, E3, E21, E21, E21]                                    |\n|blk_-1001138135617662562|[E5, E5, E5, E11, E9, E11, E9, E11, E9, E3, E3, E4, E3, E3, E4, E3, E3, E4, E3, E3, E4, E21, E21, E21]|\n|blk_-100121484143937495 |[E5, E5, E5, E11, E9, E11, E9, E11, E9, E21, E21, E21]                                                |\n|blk_-100138586392062175 |[E5, E5, E5, E11, E9, E11, E9, E11, E9]                                                               |\n|blk_-1001504241339174363|[E5, E5, E5, E11, E9, E11, E9, E11, E9, E21, E21, E21]                                                |\n|blk_-1001763494344709846|[E5, E5, E5, E11, E9, E11, E9, E11, E9, E3, E4, E3, E2, E21, E21, E21]                                |\n|blk_-1001900448998714741|[E5, E5, E5, E11, E9, E11, E9, E11, E9, E21, E21, E21]                                                |\n|blk_-1002412170036021877|[E5, E5, E5, E11, E9, E11, E9, E11, E9, E21, E21, E21]                                                |\n|blk_-1002497161352029155|[E5, E5, E5, E11, E9, E11, E9, E11, E9, E21, E21, E21]                                                |\n|blk_-1002787360783818694|[E5, E5, E5, E11, E9, E11, E9, E11, E9, E2]                                                           |\n|blk_-1003262170118050080|[E5, E5, E5, E11, E9, E11, E9, E11, E9, E2, E3, E3, E4, E3, E3, E4, E21, E21, E21]                    |\n|blk_-1003791254925854800|[E5, E5, E5, E11, E9, E11, E9, E11, E9, E21, E21, E21]                                                |\n|blk_-1004199458554820994|[E5, E5, E5, E11, E9, E11, E9, E11, E9, E21, E21, E21]                                                |\n+------------------------+------------------------------------------------------------------------------------------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, collect_list, udf, flatten\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "import re\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"BlockID Events\").getOrCreate()\n",
    "event_columns = [\"EventId\", \"EventTemplate\"]\n",
    "\n",
    "# logs_df = spark.createDataFrame(log_data, log_columns)\n",
    "logs_df = data_with_blockid\n",
    "# events_df = spark.createDataFrame(event_data, event_columns)\n",
    "events_df = df3\n",
    "\n",
    "# Show DataFrames\n",
    "logs_df.show(truncate=False)\n",
    "events_df.show(truncate=False)\n",
    "\n",
    "# Broadcast the event templates\n",
    "event_templates = {row.EventId: row.EventTemplate for row in events_df.collect()}\n",
    "event_templates_broadcast = spark.sparkContext.broadcast(event_templates)\n",
    "\n",
    "# Define UDF to extract events\n",
    "def extract_events(log_message):\n",
    "    matched_events = []\n",
    "    templates = event_templates_broadcast.value\n",
    "    for event_id, template in templates.items():\n",
    "        pattern = template.replace(\"[*]\", \".*\")\n",
    "        if re.search(pattern, log_message):\n",
    "            matched_events.append(event_id)\n",
    "    return matched_events\n",
    "\n",
    "extract_events_udf = udf(extract_events, ArrayType(StringType()))\n",
    "\n",
    "# Apply the UDF to extract events from log messages\n",
    "logs_with_events_df = logs_df.withColumn(\"events\", extract_events_udf(col(\"_c0\")))\n",
    "\n",
    "# Show intermediate DataFrame\n",
    "logs_with_events_df.show(truncate=False)\n",
    "\n",
    "# Group by blockid and aggregate the events\n",
    "aggregated_df = logs_with_events_df.groupBy(\"blockid\").agg(collect_list(\"events\").alias(\"event_lists\"))\n",
    "\n",
    "# Flatten the nested lists\n",
    "flattened_df = aggregated_df.withColumn(\"event_list\", flatten(col(\"event_lists\"))).drop(\"event_lists\")\n",
    "\n",
    "# Show final DataFrame\n",
    "flattened_df.show(truncate=False)\n",
    "\n",
    "# # Stop the Spark session\n",
    "# spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb254a29-79f7-489e-be59-68d387de34a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save the DataFrame as a CSV file\n",
    "output_path = \"dbfs:/FileStore/shared_uploads/tanmay.k.singh@sjsu.edu/block_event.csv\"\n",
    "flattened_df.write.csv(output_path, header=True, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "421f477e-689a-43bf-b1c9-60b758b0698a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Converting it into matrix for model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "128b78d6-0d8d-4075-b399-e7db69f45ff8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+------------------------------------------------------------------------------------------------------+\n|blockid                 |event_list                                                                                            |\n+------------------------+------------------------------------------------------------------------------------------------------+\n|blk_-100004553717737248 |[E5, E5, E5, E11, E9, E11, E9, E11, E9]                                                               |\n|blk_-1000218748372878626|[E5, E5, E5, E11, E9, E11, E9, E11, E9, E21, E21, E21]                                                |\n|blk_-1000284891202066880|[E5, E5, E5, E11, E9, E11, E9, E11, E9, E2, E3, E3, E4, E21, E21, E21]                                |\n|blk_-1000296688806487470|[E5, E5, E5, E11, E9, E11, E9, E11, E9, E21, E21, E21]                                                |\n|blk_-1000583943604118788|[E5, E5, E5, E11, E9, E11, E9, E11, E9]                                                               |\n|blk_-1000804803887048752|[E5, E5, E5, E11, E9, E11, E9, E11, E9, E21, E21, E21]                                                |\n|blk_-100088503271939182 |[E5, E5, E5, E11, E9, E11, E9, E11, E9]                                                               |\n|blk_-1000973735807259699|[E5, E5, E5, E11, E9, E11, E9, E11, E9, E3, E4, E3, E21, E21, E21]                                    |\n|blk_-1001138135617662562|[E5, E5, E5, E11, E9, E11, E9, E11, E9, E3, E3, E4, E3, E3, E4, E3, E3, E4, E3, E3, E4, E21, E21, E21]|\n|blk_-100121484143937495 |[E5, E5, E5, E11, E9, E11, E9, E11, E9, E21, E21, E21]                                                |\n|blk_-100138586392062175 |[E5, E5, E5, E11, E9, E11, E9, E11, E9]                                                               |\n|blk_-1001504241339174363|[E5, E5, E5, E11, E9, E11, E9, E11, E9, E21, E21, E21]                                                |\n|blk_-1001763494344709846|[E5, E5, E5, E11, E9, E11, E9, E11, E9, E3, E4, E3, E2, E21, E21, E21]                                |\n|blk_-1001900448998714741|[E5, E5, E5, E11, E9, E11, E9, E11, E9, E21, E21, E21]                                                |\n|blk_-1002412170036021877|[E5, E5, E5, E11, E9, E11, E9, E11, E9, E21, E21, E21]                                                |\n|blk_-1002497161352029155|[E5, E5, E5, E11, E9, E11, E9, E11, E9, E21, E21, E21]                                                |\n|blk_-1002787360783818694|[E5, E5, E5, E11, E9, E11, E9, E11, E9, E2]                                                           |\n|blk_-1003262170118050080|[E5, E5, E5, E11, E9, E11, E9, E11, E9, E2, E3, E3, E4, E3, E3, E4, E21, E21, E21]                    |\n|blk_-1003791254925854800|[E5, E5, E5, E11, E9, E11, E9, E11, E9, E21, E21, E21]                                                |\n|blk_-1004199458554820994|[E5, E5, E5, E11, E9, E11, E9, E11, E9, E21, E21, E21]                                                |\n+------------------------+------------------------------------------------------------------------------------------------------+\nonly showing top 20 rows\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-4054470229354866>:25\u001B[0m\n",
       "\u001B[1;32m     23\u001B[0m \u001B[38;5;66;03m# Apply the UDF to add a new column with event counts\u001B[39;00m\n",
       "\u001B[1;32m     24\u001B[0m df_with_counts \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mevent_counts\u001B[39m\u001B[38;5;124m\"\u001B[39m, count_events_udf(col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mevent_list\u001B[39m\u001B[38;5;124m\"\u001B[39m)))\n",
       "\u001B[0;32m---> 25\u001B[0m df_with_counts\u001B[38;5;241m.\u001B[39mshow(truncate\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
       "\u001B[1;32m     27\u001B[0m \u001B[38;5;66;03m# Get a list of all possible events (you can extract this from the events DataFrame)\u001B[39;00m\n",
       "\u001B[1;32m     28\u001B[0m all_events \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mE1\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mE2\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mE3\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mE4\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mE5\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mE6\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mE7\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mE8\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mE9\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mE10\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mE11\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mE12\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mE13\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mE14\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mE15\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mE16\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mE17\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mE18\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mE19\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mE20\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mE21\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mE22\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mE23\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mE24\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mE25\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mE26\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mE27\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mE28\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mE29\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:933\u001B[0m, in \u001B[0;36mDataFrame.show\u001B[0;34m(self, n, truncate, vertical)\u001B[0m\n",
       "\u001B[1;32m    924\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m:\n",
       "\u001B[1;32m    925\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n",
       "\u001B[1;32m    926\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNOT_A_BOOLEAN\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    927\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    930\u001B[0m         },\n",
       "\u001B[1;32m    931\u001B[0m     )\n",
       "\u001B[0;32m--> 933\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshowString\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mint_truncate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvertical\u001B[49m\u001B[43m)\u001B[49m)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:228\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    226\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n",
       "\u001B[1;32m    227\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m--> 228\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    229\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m    230\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n",
       "\u001B[1;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n",
       "\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n",
       "\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n",
       "\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n",
       "\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n",
       "\u001B[1;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
       "\n",
       "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o1755.showString.\n",
       ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 470.0 failed 1 times, most recent failure: Lost task 6.40 in stage 470.0 (TID 1648) (ip-10-172-213-66.us-west-2.compute.internal executor driver): com.databricks.sql.io.FileReadException: Error while reading file dbfs:REDACTED_LOCAL_PART@sjsu.edu/HDFS.log.\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logFileNameAndThrow(FileScanRDD.scala:693)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:662)\n",
       "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:786)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:488)\n",
       "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:483)\n",
       "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
       "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
       "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
       "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
       "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
       "\tat org.apache.spark.util.GroupedAsArrayIterator.hasNext(GroupedAsArrayIterator.scala:35)\n",
       "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
       "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
       "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
       "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
       "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:464)\n",
       "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:59)\n",
       "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:564)\n",
       "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2386)\n",
       "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:355)\n",
       "Caused by: java.io.IOException: getFileStatus on s3a://databricks-prod-storage-oregon/devtierprod1/8523870338369969/FileStore/shared_uploads/nikhilsarma.gudur@sjsu.edu/HDFS.log: com.amazonaws.SdkClientException: Unable to execute HTTP request: Timeout waiting for connection from pool\n",
       "\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:129)\n",
       "\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:69)\n",
       "\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV1.open(DatabricksFileSystemV1.scala:80)\n",
       "\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV1.open(DatabricksFileSystemV1.scala:89)\n",
       "\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.open(DatabricksFileSystem.scala:87)\n",
       "\tat com.databricks.spark.metrics.FileSystemWithMetrics.open(FileSystemWithMetrics.scala:346)\n",
       "\tat com.databricks.spark.metrics.FileSystemWithMetrics.$anonfun$openFileWithOptions$3(FileSystemWithMetrics.scala:369)\n",
       "\tat org.apache.hadoop.util.LambdaUtils.eval(LambdaUtils.java:52)\n",
       "\tat com.databricks.spark.metrics.FileSystemWithMetrics.openFileWithOptions(FileSystemWithMetrics.scala:369)\n",
       "\tat org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4768)\n",
       "\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:92)\n",
       "\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.<init>(HadoopFileLinesReader.scala:65)\n",
       "\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource.readFile(CSVDataSource.scala:124)\n",
       "\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.$anonfun$buildReader$2(CSVFileFormat.scala:201)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:165)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:152)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:606)\n",
       "\t... 21 more\n",
       "Caused by: java.io.InterruptedIOException: getFileStatus on s3a://databricks-prod-storage-oregon/devtierprod1/8523870338369969/FileStore/shared_uploads/nikhilsarma.gudur@sjsu.edu/HDFS.log: com.amazonaws.SdkClientException: Unable to execute HTTP request: Timeout waiting for connection from pool\n",
       "\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AUtils.translateInterruptedException(S3AUtils.java:425)\n",
       "\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:239)\n",
       "\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:197)\n",
       "\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:4356)\n",
       "\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:4287)\n",
       "\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:4166)\n",
       "\tat com.databricks.backend.daemon.data.server.backend.HadoopFSBackend.getFileStatus(HadoopFSBackend.scala:84)\n",
       "\tat com.databricks.backend.daemon.data.server.backend.RootFileSystemBackend.getFileStatus(RootFileSystemBackend.scala:63)\n",
       "\tat com.databricks.backend.daemon.data.server.handler.FileSystemRequestHandler.$anonfun$receive$3(FileSystemRequestHandler.scala:40)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:525)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:629)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:647)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:244)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:240)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.backend.daemon.data.server.handler.FileSystemRequestHandler.withAttributionContext(FileSystemRequestHandler.scala:23)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n",
       "\tat com.databricks.backend.daemon.data.server.handler.FileSystemRequestHandler.withAttributionTags(FileSystemRequestHandler.scala:23)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:624)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:534)\n",
       "\tat com.databricks.backend.daemon.data.server.handler.FileSystemRequestHandler.recordOperationWithResultTags(FileSystemRequestHandler.scala:23)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:526)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:494)\n",
       "\tat com.databricks.backend.daemon.data.server.handler.FileSystemRequestHandler.recordOperation(FileSystemRequestHandler.scala:23)\n",
       "\tat com.databricks.backend.daemon.data.server.handler.FileSystemRequestHandler.receive(FileSystemRequestHandler.scala:39)\n",
       "\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1(SessionContext.scala:54)\n",
       "\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1$adapted(SessionContext.scala:53)\n",
       "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
       "\tat com.databricks.backend.daemon.data.server.session.SessionContext.queryHandlers(SessionContext.scala:53)\n",
       "\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.$anonfun$applyOrElse$10(DbfsServerBackend.scala:436)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:244)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:240)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.applyOrElse(DbfsServerBackend.scala:436)\n",
       "\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.applyOrElse(DbfsServerBackend.scala:335)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:525)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:629)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:647)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:244)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:240)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:624)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:534)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:526)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:494)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1021)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:942)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6(JettyServer.scala:546)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6$adapted(JettyServer.scala:515)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$6(ActivityContextFactory.scala:546)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:244)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:240)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:57)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$3(ActivityContextFactory.scala:546)\n",
       "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:72)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:524)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:178)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:515)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:405)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)\n",
       "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)\n",
       "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
       "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
       "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)\n",
       "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
       "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\n",
       "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:106)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:244)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:240)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:46)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:106)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:150)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:147)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:46)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:88)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\n",
       "\tat java.lang.Thread.run(Thread.java:840)\n",
       "Caused by: java.lang.Throwable: Unable to execute HTTP request: Timeout waiting for connection from pool\n",
       "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleRetryableException(AmazonHttpClient.java:1219)\n",
       "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1165)\n",
       "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:814)\n",
       "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781)\n",
       "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755)\n",
       "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715)\n",
       "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697)\n",
       "\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561)\n",
       "\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541)\n",
       "\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5520)\n",
       "\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5467)\n",
       "\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1402)\n",
       "\tat shaded.databricks.org.apache.hadoop.fs.s3a.EnforcingDatabricksS3Client.getObjectMetadata(EnforcingDatabricksS3Client.scala:222)\n",
       "\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$7(S3AFileSystem.java:2581)\n",
       "\tat shaded.databricks.org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:434)\n",
       "\tat shaded.databricks.org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:393)\n",
       "\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2571)\n",
       "\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2539)\n",
       "\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:4342)\n",
       "\t... 125 more\n",
       "Caused by: java.lang.Throwable: Timeout waiting for connection from pool\n",
       "\tat org.apache.http.impl.conn.PoolingHttpClientConnectionManager.leaseConnection(PoolingHttpClientConnectionManager.java:316)\n",
       "\tat org.apache.http.impl.conn.PoolingHttpClientConnectionManager$1.get(PoolingHttpClientConnectionManager.java:282)\n",
       "\tat jdk.internal.reflect.GeneratedMethodAccessor80.invoke(null)\n",
       "\tat jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:568)\n",
       "\tat com.amazonaws.http.conn.ClientConnectionRequestFactory$Handler.invoke(ClientConnectionRequestFactory.java:70)\n",
       "\tat com.amazonaws.http.conn.$Proxy62.get(null)\n",
       "\tat org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:190)\n",
       "\tat org.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:186)\n",
       "\tat org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:185)\n",
       "\tat org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:83)\n",
       "\tat org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:56)\n",
       "\tat com.amazonaws.http.apache.client.impl.SdkHttpClient.execute(SdkHttpClient.java:72)\n",
       "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1346)\n",
       "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1157)\n",
       "\t... 142 more\n",
       "\n",
       "Driver stacktrace:\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3424)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3346)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3335)\n",
       "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
       "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
       "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3335)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1444)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1444)\n",
       "\tat scala.Option.foreach(Option.scala:407)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1444)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3635)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3573)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3561)\n",
       "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:51)\n",
       "Caused by: com.databricks.sql.io.FileReadException: Error while reading file dbfs:REDACTED_LOCAL_PART@sjsu.edu/HDFS.log.\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logFileNameAndThrow(FileScanRDD.scala:693)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:662)\n",
       "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:786)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:488)\n",
       "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:483)\n",
       "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
       "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
       "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
       "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
       "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
       "\tat org.apache.spark.util.GroupedAsArrayIterator.hasNext(GroupedAsArrayIterator.scala:35)\n",
       "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
       "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
       "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
       "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
       "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:464)\n",
       "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:59)\n",
       "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:564)\n",
       "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2386)\n",
       "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:355)\n",
       "Caused by: java.io.IOException: getFileStatus on s3a://databricks-prod-storage-oregon/devtierprod1/8523870338369969/FileStore/shared_uploads/nikhilsarma.gudur@sjsu.edu/HDFS.log: com.amazonaws.SdkClientException: Unable to execute HTTP request: Timeout waiting for connection from pool\n",
       "\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:129)\n",
       "\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:69)\n",
       "\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV1.open(DatabricksFileSystemV1.scala:80)\n",
       "\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV1.open(DatabricksFileSystemV1.scala:89)\n",
       "\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.open(DatabricksFileSystem.scala:87)\n",
       "\tat com.databricks.spark.metrics.FileSystemWithMetrics.open(FileSystemWithMetrics.scala:346)\n",
       "\tat com.databricks.spark.metrics.FileSystemWithMetrics.$anonfun$openFileWithOptions$3(FileSystemWithMetrics.scala:369)\n",
       "\tat org.apache.hadoop.util.LambdaUtils.eval(LambdaUtils.java:52)\n",
       "\tat com.databricks.spark.metrics.FileSystemWithMetrics.openFileWithOptions(FileSystemWithMetrics.scala:369)\n",
       "\tat org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4768)\n",
       "\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:92)\n",
       "\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.<init>(HadoopFileLinesReader.scala:65)\n",
       "\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource.readFile(CSVDataSource.scala:124)\n",
       "\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.$anonfun$buildReader$2(CSVFileFormat.scala:201)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:165)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:152)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:606)\n",
       "\t... 21 more\n",
       "Caused by: java.io.InterruptedIOException: getFileStatus on s3a://databricks-prod-storage-oregon/devtierprod1/8523870338369969/FileStore/shared_uploads/nikhilsarma.gudur@sjsu.edu/HDFS.log: com.amazonaws.SdkClientException: Unable to execute HTTP request: Timeout waiting for connection from pool\n",
       "\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AUtils.translateInterruptedException(S3AUtils.java:425)\n",
       "\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:239)\n",
       "\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:197)\n",
       "\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:4356)\n",
       "\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:4287)\n",
       "\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:4166)\n",
       "\tat com.databricks.backend.daemon.data.server.backend.HadoopFSBackend.getFileStatus(HadoopFSBackend.scala:84)\n",
       "\tat com.databricks.backend.daemon.data.server.backend.RootFileSystemBackend.getFileStatus(RootFileSystemBackend.scala:63)\n",
       "\tat com.databricks.backend.daemon.data.server.handler.FileSystemRequestHandler.$anonfun$receive$3(FileSystemRequestHandler.scala:40)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:525)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:629)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:647)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:244)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:240)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.backend.daemon.data.server.handler.FileSystemRequestHandler.withAttributionContext(FileSystemRequestHandler.scala:23)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n",
       "\tat com.databricks.backend.daemon.data.server.handler.FileSystemRequestHandler.withAttributionTags(FileSystemRequestHandler.scala:23)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:624)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:534)\n",
       "\tat com.databricks.backend.daemon.data.server.handler.FileSystemRequestHandler.recordOperationWithResultTags(FileSystemRequestHandler.scala:23)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:526)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:494)\n",
       "\tat com.databricks.backend.daemon.data.server.handler.FileSystemRequestHandler.recordOperation(FileSystemRequestHandler.scala:23)\n",
       "\tat com.databricks.backend.daemon.data.server.handler.FileSystemRequestHandler.receive(FileSystemRequestHandler.scala:39)\n",
       "\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1(SessionContext.scala:54)\n",
       "\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1$adapted(SessionContext.scala:53)\n",
       "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
       "\tat com.databricks.backend.daemon.data.server.session.SessionContext.queryHandlers(SessionContext.scala:53)\n",
       "\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.$anonfun$applyOrElse$10(DbfsServerBackend.scala:436)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:244)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:240)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.applyOrElse(DbfsServerBackend.scala:436)\n",
       "\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.applyOrElse(DbfsServerBackend.scala:335)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:525)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:629)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:647)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:244)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:240)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:624)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:534)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:526)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:494)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1021)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:942)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6(JettyServer.scala:546)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6$adapted(JettyServer.scala:515)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$6(ActivityContextFactory.scala:546)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:244)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:240)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:57)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$3(ActivityContextFactory.scala:546)\n",
       "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:72)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:524)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:178)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:515)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:405)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)\n",
       "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)\n",
       "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
       "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
       "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)\n",
       "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
       "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\n",
       "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:106)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:244)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:240)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:46)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:106)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:150)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:147)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:46)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:88)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\n",
       "\tat java.lang.Thread.run(Thread.java:840)\n",
       "Caused by: java.lang.Throwable: Unable to execute HTTP request: Timeout waiting for connection from pool\n",
       "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleRetryableException(AmazonHttpClient.java:1219)\n",
       "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1165)\n",
       "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:814)\n",
       "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781)\n",
       "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755)\n",
       "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715)\n",
       "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697)\n",
       "\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561)\n",
       "\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541)\n",
       "\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5520)\n",
       "\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5467)\n",
       "\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1402)\n",
       "\tat shaded.databricks.org.apache.hadoop.fs.s3a.EnforcingDatabricksS3Client.getObjectMetadata(EnforcingDatabricksS3Client.scala:222)\n",
       "\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$7(S3AFileSystem.java:2581)\n",
       "\tat shaded.databricks.org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:434)\n",
       "\tat shaded.databricks.org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:393)\n",
       "\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2571)\n",
       "\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2539)\n",
       "\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:4342)\n",
       "\t... 125 more\n",
       "Caused by: java.lang.Throwable: Timeout waiting for connection from pool\n",
       "\tat org.apache.http.impl.conn.PoolingHttpClientConnectionManager.leaseConnection(PoolingHttpClientConnectionManager.java:316)\n",
       "\tat org.apache.http.impl.conn.PoolingHttpClientConnectionManager$1.get(PoolingHttpClientConnectionManager.java:282)\n",
       "\tat jdk.internal.reflect.GeneratedMethodAccessor80.invoke(null)\n",
       "\tat jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:568)\n",
       "\tat com.amazonaws.http.conn.ClientConnectionRequestFactory$Handler.invoke(ClientConnectionRequestFactory.java:70)\n",
       "\tat com.amazonaws.http.conn.$Proxy62.get(null)\n",
       "\tat org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:190)\n",
       "\tat org.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:186)\n",
       "\tat org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:185)\n",
       "\tat org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:83)\n",
       "\tat org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:56)\n",
       "\tat com.amazonaws.http.apache.client.impl.SdkHttpClient.execute(SdkHttpClient.java:72)\n",
       "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1346)\n",
       "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1157)\n",
       "\t... 142 more\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\nFile \u001B[0;32m<command-4054470229354866>:25\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;66;03m# Apply the UDF to add a new column with event counts\u001B[39;00m\n\u001B[1;32m     24\u001B[0m df_with_counts \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mevent_counts\u001B[39m\u001B[38;5;124m\"\u001B[39m, count_events_udf(col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mevent_list\u001B[39m\u001B[38;5;124m\"\u001B[39m)))\n\u001B[0;32m---> 25\u001B[0m df_with_counts\u001B[38;5;241m.\u001B[39mshow(truncate\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m     27\u001B[0m \u001B[38;5;66;03m# Get a list of all possible events (you can extract this from the events DataFrame)\u001B[39;00m\n\u001B[1;32m     28\u001B[0m all_events \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mE1\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mE2\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mE3\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mE4\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mE5\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mE6\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mE7\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mE8\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mE9\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mE10\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mE11\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mE12\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mE13\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mE14\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mE15\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mE16\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mE17\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mE18\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mE19\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mE20\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mE21\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mE22\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mE23\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mE24\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mE25\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mE26\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mE27\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mE28\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mE29\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:933\u001B[0m, in \u001B[0;36mDataFrame.show\u001B[0;34m(self, n, truncate, vertical)\u001B[0m\n\u001B[1;32m    924\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m:\n\u001B[1;32m    925\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n\u001B[1;32m    926\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNOT_A_BOOLEAN\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    927\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    930\u001B[0m         },\n\u001B[1;32m    931\u001B[0m     )\n\u001B[0;32m--> 933\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshowString\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mint_truncate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvertical\u001B[49m\u001B[43m)\u001B[49m)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:228\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    226\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m    227\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 228\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    229\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    230\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[1;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n\n\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o1755.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 470.0 failed 1 times, most recent failure: Lost task 6.40 in stage 470.0 (TID 1648) (ip-10-172-213-66.us-west-2.compute.internal executor driver): com.databricks.sql.io.FileReadException: Error while reading file dbfs:REDACTED_LOCAL_PART@sjsu.edu/HDFS.log.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logFileNameAndThrow(FileScanRDD.scala:693)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:662)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:786)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:488)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:483)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.GroupedAsArrayIterator.hasNext(GroupedAsArrayIterator.scala:35)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:464)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:59)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:564)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2386)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:355)\nCaused by: java.io.IOException: getFileStatus on s3a://databricks-prod-storage-oregon/devtierprod1/8523870338369969/FileStore/shared_uploads/nikhilsarma.gudur@sjsu.edu/HDFS.log: com.amazonaws.SdkClientException: Unable to execute HTTP request: Timeout waiting for connection from pool\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:129)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:69)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV1.open(DatabricksFileSystemV1.scala:80)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV1.open(DatabricksFileSystemV1.scala:89)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.open(DatabricksFileSystem.scala:87)\n\tat com.databricks.spark.metrics.FileSystemWithMetrics.open(FileSystemWithMetrics.scala:346)\n\tat com.databricks.spark.metrics.FileSystemWithMetrics.$anonfun$openFileWithOptions$3(FileSystemWithMetrics.scala:369)\n\tat org.apache.hadoop.util.LambdaUtils.eval(LambdaUtils.java:52)\n\tat com.databricks.spark.metrics.FileSystemWithMetrics.openFileWithOptions(FileSystemWithMetrics.scala:369)\n\tat org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4768)\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:92)\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.<init>(HadoopFileLinesReader.scala:65)\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource.readFile(CSVDataSource.scala:124)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.$anonfun$buildReader$2(CSVFileFormat.scala:201)\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:165)\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:152)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:606)\n\t... 21 more\nCaused by: java.io.InterruptedIOException: getFileStatus on s3a://databricks-prod-storage-oregon/devtierprod1/8523870338369969/FileStore/shared_uploads/nikhilsarma.gudur@sjsu.edu/HDFS.log: com.amazonaws.SdkClientException: Unable to execute HTTP request: Timeout waiting for connection from pool\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AUtils.translateInterruptedException(S3AUtils.java:425)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:239)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:197)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:4356)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:4287)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:4166)\n\tat com.databricks.backend.daemon.data.server.backend.HadoopFSBackend.getFileStatus(HadoopFSBackend.scala:84)\n\tat com.databricks.backend.daemon.data.server.backend.RootFileSystemBackend.getFileStatus(RootFileSystemBackend.scala:63)\n\tat com.databricks.backend.daemon.data.server.handler.FileSystemRequestHandler.$anonfun$receive$3(FileSystemRequestHandler.scala:40)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:525)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:629)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:647)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:244)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:240)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n\tat com.databricks.backend.daemon.data.server.handler.FileSystemRequestHandler.withAttributionContext(FileSystemRequestHandler.scala:23)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n\tat com.databricks.backend.daemon.data.server.handler.FileSystemRequestHandler.withAttributionTags(FileSystemRequestHandler.scala:23)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:624)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:534)\n\tat com.databricks.backend.daemon.data.server.handler.FileSystemRequestHandler.recordOperationWithResultTags(FileSystemRequestHandler.scala:23)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:526)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:494)\n\tat com.databricks.backend.daemon.data.server.handler.FileSystemRequestHandler.recordOperation(FileSystemRequestHandler.scala:23)\n\tat com.databricks.backend.daemon.data.server.handler.FileSystemRequestHandler.receive(FileSystemRequestHandler.scala:39)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1(SessionContext.scala:54)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1$adapted(SessionContext.scala:53)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.queryHandlers(SessionContext.scala:53)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.$anonfun$applyOrElse$10(DbfsServerBackend.scala:436)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:244)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:240)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.applyOrElse(DbfsServerBackend.scala:436)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.applyOrElse(DbfsServerBackend.scala:335)\n\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)\n\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:525)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:629)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:647)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:244)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:240)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:624)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:534)\n\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:526)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:494)\n\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)\n\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1021)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:942)\n\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6(JettyServer.scala:546)\n\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6$adapted(JettyServer.scala:515)\n\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$6(ActivityContextFactory.scala:546)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:244)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:240)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:57)\n\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$3(ActivityContextFactory.scala:546)\n\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:72)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:524)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:178)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:515)\n\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:405)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)\n\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)\n\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n\tat org.eclipse.jetty.server.Server.handle(Server.java:516)\n\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)\n\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)\n\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)\n\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\n\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)\n\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:106)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:244)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:240)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:46)\n\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:106)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:150)\n\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:147)\n\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:46)\n\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:88)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\n\tat java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.Throwable: Unable to execute HTTP request: Timeout waiting for connection from pool\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleRetryableException(AmazonHttpClient.java:1219)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1165)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:814)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5520)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5467)\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1402)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.EnforcingDatabricksS3Client.getObjectMetadata(EnforcingDatabricksS3Client.scala:222)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$7(S3AFileSystem.java:2581)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:434)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:393)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2571)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2539)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:4342)\n\t... 125 more\nCaused by: java.lang.Throwable: Timeout waiting for connection from pool\n\tat org.apache.http.impl.conn.PoolingHttpClientConnectionManager.leaseConnection(PoolingHttpClientConnectionManager.java:316)\n\tat org.apache.http.impl.conn.PoolingHttpClientConnectionManager$1.get(PoolingHttpClientConnectionManager.java:282)\n\tat jdk.internal.reflect.GeneratedMethodAccessor80.invoke(null)\n\tat jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:568)\n\tat com.amazonaws.http.conn.ClientConnectionRequestFactory$Handler.invoke(ClientConnectionRequestFactory.java:70)\n\tat com.amazonaws.http.conn.$Proxy62.get(null)\n\tat org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:190)\n\tat org.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:186)\n\tat org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:185)\n\tat org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:83)\n\tat org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:56)\n\tat com.amazonaws.http.apache.client.impl.SdkHttpClient.execute(SdkHttpClient.java:72)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1346)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1157)\n\t... 142 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3424)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3346)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3335)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3335)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1444)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1444)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1444)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3635)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3573)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3561)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:51)\nCaused by: com.databricks.sql.io.FileReadException: Error while reading file dbfs:REDACTED_LOCAL_PART@sjsu.edu/HDFS.log.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logFileNameAndThrow(FileScanRDD.scala:693)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:662)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:786)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:488)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:483)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.util.GroupedAsArrayIterator.hasNext(GroupedAsArrayIterator.scala:35)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:464)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:59)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:564)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2386)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:355)\nCaused by: java.io.IOException: getFileStatus on s3a://databricks-prod-storage-oregon/devtierprod1/8523870338369969/FileStore/shared_uploads/nikhilsarma.gudur@sjsu.edu/HDFS.log: com.amazonaws.SdkClientException: Unable to execute HTTP request: Timeout waiting for connection from pool\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:129)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:69)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV1.open(DatabricksFileSystemV1.scala:80)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV1.open(DatabricksFileSystemV1.scala:89)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.open(DatabricksFileSystem.scala:87)\n\tat com.databricks.spark.metrics.FileSystemWithMetrics.open(FileSystemWithMetrics.scala:346)\n\tat com.databricks.spark.metrics.FileSystemWithMetrics.$anonfun$openFileWithOptions$3(FileSystemWithMetrics.scala:369)\n\tat org.apache.hadoop.util.LambdaUtils.eval(LambdaUtils.java:52)\n\tat com.databricks.spark.metrics.FileSystemWithMetrics.openFileWithOptions(FileSystemWithMetrics.scala:369)\n\tat org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4768)\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:92)\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.<init>(HadoopFileLinesReader.scala:65)\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource.readFile(CSVDataSource.scala:124)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.$anonfun$buildReader$2(CSVFileFormat.scala:201)\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:165)\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:152)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:606)\n\t... 21 more\nCaused by: java.io.InterruptedIOException: getFileStatus on s3a://databricks-prod-storage-oregon/devtierprod1/8523870338369969/FileStore/shared_uploads/nikhilsarma.gudur@sjsu.edu/HDFS.log: com.amazonaws.SdkClientException: Unable to execute HTTP request: Timeout waiting for connection from pool\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AUtils.translateInterruptedException(S3AUtils.java:425)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:239)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:197)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:4356)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:4287)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:4166)\n\tat com.databricks.backend.daemon.data.server.backend.HadoopFSBackend.getFileStatus(HadoopFSBackend.scala:84)\n\tat com.databricks.backend.daemon.data.server.backend.RootFileSystemBackend.getFileStatus(RootFileSystemBackend.scala:63)\n\tat com.databricks.backend.daemon.data.server.handler.FileSystemRequestHandler.$anonfun$receive$3(FileSystemRequestHandler.scala:40)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:525)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:629)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:647)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:244)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:240)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n\tat com.databricks.backend.daemon.data.server.handler.FileSystemRequestHandler.withAttributionContext(FileSystemRequestHandler.scala:23)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n\tat com.databricks.backend.daemon.data.server.handler.FileSystemRequestHandler.withAttributionTags(FileSystemRequestHandler.scala:23)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:624)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:534)\n\tat com.databricks.backend.daemon.data.server.handler.FileSystemRequestHandler.recordOperationWithResultTags(FileSystemRequestHandler.scala:23)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:526)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:494)\n\tat com.databricks.backend.daemon.data.server.handler.FileSystemRequestHandler.recordOperation(FileSystemRequestHandler.scala:23)\n\tat com.databricks.backend.daemon.data.server.handler.FileSystemRequestHandler.receive(FileSystemRequestHandler.scala:39)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1(SessionContext.scala:54)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1$adapted(SessionContext.scala:53)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.queryHandlers(SessionContext.scala:53)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.$anonfun$applyOrElse$10(DbfsServerBackend.scala:436)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:244)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:240)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.applyOrElse(DbfsServerBackend.scala:436)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$4.applyOrElse(DbfsServerBackend.scala:335)\n\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)\n\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:525)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:629)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:647)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:244)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:240)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:624)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:534)\n\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:526)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:494)\n\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)\n\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1021)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:942)\n\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6(JettyServer.scala:546)\n\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6$adapted(JettyServer.scala:515)\n\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$6(ActivityContextFactory.scala:546)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:244)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:240)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:57)\n\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$3(ActivityContextFactory.scala:546)\n\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:72)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:524)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:178)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:515)\n\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:405)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)\n\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)\n\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n\tat org.eclipse.jetty.server.Server.handle(Server.java:516)\n\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)\n\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)\n\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)\n\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\n\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)\n\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:106)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:244)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:240)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:46)\n\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:106)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:150)\n\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:147)\n\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:46)\n\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:88)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\n\tat java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.Throwable: Unable to execute HTTP request: Timeout waiting for connection from pool\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleRetryableException(AmazonHttpClient.java:1219)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1165)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:814)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5520)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5467)\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1402)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.EnforcingDatabricksS3Client.getObjectMetadata(EnforcingDatabricksS3Client.scala:222)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$7(S3AFileSystem.java:2581)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:434)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:393)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2571)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2539)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:4342)\n\t... 125 more\nCaused by: java.lang.Throwable: Timeout waiting for connection from pool\n\tat org.apache.http.impl.conn.PoolingHttpClientConnectionManager.leaseConnection(PoolingHttpClientConnectionManager.java:316)\n\tat org.apache.http.impl.conn.PoolingHttpClientConnectionManager$1.get(PoolingHttpClientConnectionManager.java:282)\n\tat jdk.internal.reflect.GeneratedMethodAccessor80.invoke(null)\n\tat jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:568)\n\tat com.amazonaws.http.conn.ClientConnectionRequestFactory$Handler.invoke(ClientConnectionRequestFactory.java:70)\n\tat com.amazonaws.http.conn.$Proxy62.get(null)\n\tat org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:190)\n\tat org.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:186)\n\tat org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:185)\n\tat org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:83)\n\tat org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:56)\n\tat com.amazonaws.http.apache.client.impl.SdkHttpClient.execute(SdkHttpClient.java:72)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1346)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1157)\n\t... 142 more\n",
       "errorSummary": "org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 470.0 failed 1 times, most recent failure: Lost task 6.40 in stage 470.0 (TID 1648) (ip-10-172-213-66.us-west-2.compute.internal executor driver): com.databricks.sql.io.FileReadException: Error while reading file dbfs:REDACTED_LOCAL_PART@sjsu.edu/HDFS.log.",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, collect_list, flatten, udf, coalesce, lit\n",
    "from pyspark.sql.types import StringType, IntegerType, MapType\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"BlockID Events Transformation\").getOrCreate()\n",
    "\n",
    "df = flattened_df\n",
    "df.show(truncate=False)\n",
    "\n",
    "# Define UDF to count events\n",
    "def count_events(event_list):\n",
    "    event_counts = {}\n",
    "    for event in event_list:\n",
    "        if event in event_counts:\n",
    "            event_counts[event] += 1\n",
    "        else:\n",
    "            event_counts[event] = 1\n",
    "    return event_counts\n",
    "\n",
    "count_events_udf = udf(count_events, MapType(StringType(), IntegerType()))\n",
    "\n",
    "# Apply the UDF to add a new column with event counts\n",
    "df_with_counts = df.withColumn(\"event_counts\", count_events_udf(col(\"event_list\")))\n",
    "\n",
    "\n",
    "# Get a list of all possible events (you can extract this from the events DataFrame)\n",
    "all_events = [\"E1\", \"E2\", \"E3\", \"E4\", \"E5\", \"E6\", \"E7\", \"E8\", \"E9\", \"E10\", \"E11\", \"E12\", \"E13\", \"E14\", \"E15\", \"E16\", \"E17\", \"E18\", \"E19\", \"E20\", \"E21\", \"E22\", \"E23\", \"E24\", \"E25\", \"E26\", \"E27\", \"E28\", \"E29\"]\n",
    "\n",
    "# Create columns for each event with count values\n",
    "for event in all_events:\n",
    "    df_with_counts = df_with_counts.withColumn(event, coalesce(col(\"event_counts\").getItem(event), lit(0)))\n",
    "\n",
    "# Drop the event_counts column\n",
    "final_df = df_with_counts.drop(\"event_counts\")\n",
    "\n",
    "# Show the final DataFrame\n",
    "final_df.show(truncate=False)\n",
    "\n",
    "# Save the final DataFrame as a CSV file\n",
    "# output_path = \"/mnt/data/blockid_events_transformed\"\n",
    "# final_df.write.csv(output_path, header=True, mode=\"overwrite\")\n",
    "\n",
    "# # Stop the Spark session\n",
    "# spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bff3090e-d1a1-49b7-8354-48ffbb4f408d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11175629\n"
     ]
    }
   ],
   "source": [
    "print(data_with_blockid.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2976791-1524-4390-bed4-60075a980ea1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------+\n|_c0                                                                                                                                                                |blockid                 |\n+-------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------+\n|081109 203518 143 INFO dfs.DataNode$DataXceiver: Receiving block blk_-1608999687919862906 src: /10.250.19.102:54106 dest: /10.250.19.102:50010                     |blk_-1608999687919862906|\n|081109 203518 35 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /mnt/hadoop/mapred/system/job_200811092030_0001/job.jar. blk_-1608999687919862906         |blk_-1608999687919862906|\n|081109 203519 143 INFO dfs.DataNode$DataXceiver: Receiving block blk_-1608999687919862906 src: /10.250.10.6:40524 dest: /10.250.10.6:50010                         |blk_-1608999687919862906|\n|081109 203519 145 INFO dfs.DataNode$DataXceiver: Receiving block blk_-1608999687919862906 src: /10.250.14.224:42420 dest: /10.250.14.224:50010                     |blk_-1608999687919862906|\n|081109 203519 145 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_-1608999687919862906 terminating                                              |blk_-1608999687919862906|\n|081109 203519 145 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_-1608999687919862906 terminating                                              |blk_-1608999687919862906|\n|081109 203519 145 INFO dfs.DataNode$PacketResponder: Received block blk_-1608999687919862906 of size 91178 from /10.250.10.6                                       |blk_-1608999687919862906|\n|081109 203519 145 INFO dfs.DataNode$PacketResponder: Received block blk_-1608999687919862906 of size 91178 from /10.250.19.102                                     |blk_-1608999687919862906|\n|081109 203519 147 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_-1608999687919862906 terminating                                              |blk_-1608999687919862906|\n|081109 203519 147 INFO dfs.DataNode$PacketResponder: Received block blk_-1608999687919862906 of size 91178 from /10.250.14.224                                     |blk_-1608999687919862906|\n|081109 203519 29 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.10.6:50010 is added to blk_-1608999687919862906 size 91178      |blk_-1608999687919862906|\n|081109 203519 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.111.209:50010 is added to blk_-1608999687919862906 size 91178   |blk_-1608999687919862906|\n|081109 203519 31 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.14.224:50010 is added to blk_-1608999687919862906 size 91178    |blk_-1608999687919862906|\n|081109 203521 143 INFO dfs.DataNode$DataXceiver: Received block blk_-1608999687919862906 src: /10.251.215.16:52002 dest: /10.251.215.16:50010 of size 91178        |blk_-1608999687919862906|\n|081109 203521 143 INFO dfs.DataNode$DataXceiver: Receiving block blk_-1608999687919862906 src: /10.251.215.16:52002 dest: /10.251.215.16:50010                     |blk_-1608999687919862906|\n|081109 203521 147 INFO dfs.DataNode$DataTransfer: 10.250.14.224:50010:Transmitted block blk_-1608999687919862906 to /10.251.215.16:50010                           |blk_-1608999687919862906|\n|081109 203521 147 INFO dfs.DataNode$DataXceiver: Received block blk_-1608999687919862906 src: /10.250.14.224:35754 dest: /10.250.14.224:50010 of size 91178        |blk_-1608999687919862906|\n|081109 203521 147 INFO dfs.DataNode$DataXceiver: Receiving block blk_-1608999687919862906 src: /10.250.14.224:35754 dest: /10.250.14.224:50010                     |blk_-1608999687919862906|\n|081109 203521 19 INFO dfs.DataNode: 10.250.14.224:50010 Starting thread to transfer block blk_-1608999687919862906 to 10.251.215.16:50010                          |blk_-1608999687919862906|\n|081109 203521 19 INFO dfs.FSNamesystem: BLOCK* ask 10.250.14.224:50010 to replicate blk_-1608999687919862906 to datanode(s) 10.251.215.16:50010 10.251.71.193:50010|blk_-1608999687919862906|\n+-------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "data_with_blockid.filter(col(\"blockid\") == 'blk_-1608999687919862906').show( truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a3182e3-0a1b-4968-abab-9b7d1e284f33",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n|             blockid|count|\n+--------------------+-----+\n|blk_2329219899967...|   38|\n|blk_7573381576932...|   38|\n|blk_-514328661767...|   38|\n|blk_-315541563754...|   25|\n|blk_-626551751220...|   25|\n|blk_-407011522237...|   25|\n|blk_-189452694820...|   25|\n|blk_-243754728424...|   27|\n|blk_-562137042848...|   25|\n|blk_8243751147898...|   25|\n|blk_-398207552508...|   25|\n|blk_-299506639704...|   24|\n|blk_2176961593541...|   25|\n|blk_4229018881350...|   25|\n|blk_3363721847495...|   25|\n|blk_7661137288132...|   25|\n|blk_-772145702470...|   25|\n|blk_8720307910510...|   25|\n|blk_8245176888463...|   25|\n|blk_-144962563959...|   25|\n+--------------------+-----+\nonly showing top 20 rows\n\nNone\n"
     ]
    }
   ],
   "source": [
    "print(data_with_blockid.groupby(\"blockid\").count().show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ccaa8e8-ea38-4424-bcc5-0c130c7dc0d9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[25]: 575061"
     ]
    }
   ],
   "source": [
    "data_with_blockid.select(\"blockid\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13941cc9-214d-4e32-9f1e-d05f98b68825",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "hdfs_logs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e596f847-a9c5-4f36-9a4d-a38c50a3c10d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7d2d269-7f83-4653-96be-bce4b9fe136b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+----+--------------------+--------------------+-------+\n|             BlockId|  Label|Type|            Features|        TimeInterval|Latency|\n+--------------------+-------+----+--------------------+--------------------+-------+\n|blk_-160899968791...|Success|null|[E5,E22,E5,E5,E11...|[0.0, 1.0, 0.0, 0...|   3802|\n|blk_7503483334202...|Success|null|[E5,E5,E22,E5,E11...|[0.0, 0.0, 1.0, 0...|   3802|\n|blk_-354458337728...|   Fail|  21|[E5,E22,E5,E5,E11...|[0.0, 1.0, 0.0, 0...|   3797|\n|blk_-907399258668...|Success|null|[E5,E22,E5,E5,E11...|[0.0, 1.0, 0.0, 0...|  50448|\n|blk_7854771516489...|Success|null|[E5,E5,E22,E5,E11...|[0.0, 0.0, 1.0, 4...|  50583|\n|blk_1717858812220...|Success|null|[E5,E5,E22,E5,E11...|[0.0, 0.0, 11.0, ...|  50458|\n|blk_-251961732037...|Success|null|[E5,E22,E5,E5,E11...|[0.0, 1.0, 9.0, 4...|  50523|\n|blk_7063315473424...|Success|null|[E5,E5,E5,E22,E11...|[1.0, 0.0, 0.0, 5...|  50818|\n|blk_8586544123689...|Success|null|[E5,E5,E5,E22,E11...|[1.0, 0.0, 0.0, 3...|  50795|\n|blk_2765344736980...|Success|null|[E5,E5,E22,E5,E11...|[0.0, 0.0, 1.0, 2...|  50528|\n|blk_-290049055749...|Success|null|[E5,E5,E22,E5,E11...|[0.0, 0.0, 7.0, 4...|  50440|\n|blk_-502732577314...|Success|null|[E5,E5,E22,E5,E9,...|[0.0, 0.0, 3.0, 3...|  50431|\n|blk_4394112519745...|Success|null|[E5,E5,E22,E5,E11...|[0.0, 0.0, 7.0, 3...|  50529|\n|blk_3640100967125...|Success|null|[E5,E5,E22,E5,E11...|[0.0, 0.0, 1.0, 4...|  50419|\n|blk_-401156444932...|Success|null|[E5,E5,E22,E5,E11...|[0.0, 0.0, 1.0, 3...|  50495|\n|blk_-853131033556...|   Fail|   4|[E5,E22,E5,E5,E11...|[0.0, 2.0, 0.0, 5...|  51107|\n|blk_-340992364514...|Success|null|[E5,E22,E5,E5,E11...|[0.0, 1.0, 9.0, 5...|  50443|\n|blk_3974948352784...|Success|null|[E5,E5,E22,E5,E11...|[0.0, 0.0, 3.0, 6...|  50824|\n|blk_5647760196018...|Success|null|[E5,E5,E5,E22,E11...|[0.0, 0.0, 0.0, 3...|  50556|\n|blk_-202775138379...|Success|null|[E5,E22,E5,E5,E11...|[0.0, 1.0, 10.0, ...|  50523|\n+--------------------+-------+----+--------------------+--------------------+-------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0db6986e-4e63-4c7d-b01a-9df9c27edf38",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "df4 = df4.withColumn(\"Features\", split(col(\"Features\"), \",\"))\n",
    "\n",
    "exploded_df = df4.select(\"BlockId\", \"Label\", \"Type\", explode(\n",
    "    \"Features\").alias(\"Event\"), \"TimeInterval\", \"Latency\")\n",
    "exploded_df = exploded_df.withColumn(\n",
    "    \"Event\", regexp_extract(col(\"Event\"), r\"E\\d+\", 0))\n",
    "\n",
    "event_counts_df = exploded_df.groupBy(\n",
    "    \"BlockId\", \"Label\", \"Type\", \"Event\", \"TimeInterval\", \"Latency\").agg(count(\"Event\").alias(\"Count\"))\n",
    "\n",
    "pivoted_df = event_counts_df.groupBy(\"BlockId\", \"Label\", \"Type\", \"TimeInterval\", \"Latency\").pivot(\n",
    "    \"Event\").agg({\"Count\": \"first\"}).na.fill(0)\n",
    "\n",
    "pivoted_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a98473e-276f-4154-ac3a-f13eeb8fc467",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import ArrayType, FloatType, StructType, StructField, IntegerType\n",
    "import random\n",
    "\n",
    "# Assemble feature columns into a single vector\n",
    "assembler = VectorAssembler(inputCols=[col for col in df.columns if col != 'Label'], outputCol=\"features\")\n",
    "df_assembled = assembler.transform(df)\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=True)\n",
    "scaler_model = scaler.fit(df_assembled)\n",
    "df_scaled = scaler_model.transform(df_assembled)\n",
    "\n",
    "# Split the data into majority and minority classes\n",
    "majority_df = df_scaled.filter(df_scaled.Label == 1)\n",
    "minority_df = df_scaled.filter(df_scaled.Label == 0)\n",
    "\n",
    "# Function to generate synthetic samples\n",
    "def generate_samples(minority_class, n_samples):\n",
    "    synthetic_samples = []\n",
    "    minority_data = minority_class.collect()\n",
    "    for _ in range(n_samples):\n",
    "        sample1, sample2 = random.sample(minority_data, 2)\n",
    "        weight = random.random()\n",
    "        synthetic_sample = DenseVector([weight * x + (1 - weight) * y for x, y in zip(sample1['scaledFeatures'], sample2['scaledFeatures'])])\n",
    "        synthetic_samples.append((0, synthetic_sample))\n",
    "    return synthetic_samples\n",
    "\n",
    "# Number of synthetic samples needed\n",
    "n_majority = majority_df.count()\n",
    "n_minority = minority_df.count()\n",
    "n_synthetic_samples = n_majority - n_minority\n",
    "\n",
    "# Generate synthetic samples\n",
    "synthetic_samples = generate_samples(minority_df, n_synthetic_samples)\n",
    "\n",
    "# Create a DataFrame for synthetic samples\n",
    "schema = StructType([\n",
    "    StructField(\"Label\", IntegerType(), False),\n",
    "    StructField(\"scaledFeatures\", ArrayType(FloatType()), False)\n",
    "])\n",
    "synthetic_df = spark.createDataFrame(synthetic_samples, schema)\n",
    "\n",
    "# Combine the original majority class with the minority class and synthetic samples\n",
    "balanced_df = majority_df.union(minority_df).union(synthetic_df)\n",
    "\n",
    "# Convert scaledFeatures back to individual columns\n",
    "def vector_to_columns(scaled_features):\n",
    "    return scaled_features.tolist()\n",
    "\n",
    "vector_to_columns_udf = udf(vector_to_columns, ArrayType(FloatType()))\n",
    "\n",
    "# Add individual columns to the DataFrame\n",
    "balanced_df = balanced_df.withColumn(\"features_list\", vector_to_columns_udf(\"scaledFeatures\"))\n",
    "\n",
    "# Extract feature column names\n",
    "feature_columns = [col for col in df.columns if col != 'Label']\n",
    "for i, col_name in enumerate(feature_columns):\n",
    "    balanced_df = balanced_df.withColumn(col_name, col(\"features_list\")[i])\n",
    "\n",
    "# Drop unnecessary columns\n",
    "final_df = balanced_df.drop(\"features\").drop(\"scaledFeatures\").drop(\"features_list\")\n",
    "\n",
    "# Separate target and features\n",
    "final_df = final_df.select([\"Label\"] + feature_columns)\n",
    "\n",
    "# Show the final DataFrame\n",
    "final_df.show()\n",
    "\n",
    "# Now `final_df` contains the balanced dataset with target variable 'Label' and features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40eeefe5-51b4-4302-9ea0-2d19f8398d55",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "The spark context has stopped and the driver is restarting. Your notebook will be automatically reattached.",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ffd85d3-7d0a-41ed-bbc1-b4beb55ffa68",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Big Data",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
